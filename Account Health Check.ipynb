{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "u5v7dxae47kprymv5xgo",
   "authorId": "6025621082839",
   "authorName": "ADMIN",
   "authorEmail": "jonghyun.hong@snowflake.com",
   "sessionId": "4a9310ea-e80a-40d3-afbf-b8495554f4b1",
   "lastEditTime": 1754667100425
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11711d21-821a-488a-871f-bb42b019ccb3",
   "metadata": {
    "name": "Header",
    "collapsed": false
   },
   "source": "# Snowflake Account Usage\n\n### 이 앱은 여러분의 Snowflake 계정에 있는 account_usage 스키마를 기반으로 작동하도록 개발되었습니다. \n\n자세한 정보는 아래 Snowflake 공식 문서 페이지를 참조하십시오.  \nhttps://docs.snowflake.com/en/sql-reference/account-usage#account-usage-views\n\nSnowflake SE팀의 Nikhil Kolur & Ashish Patel 개발한 Streamlit 버전을 Snowflake Notebooks로 전환하였습니다.  \nhttps://medium.com/snowflake/monitoring-snowflake-with-streamlit-in-snowflake-sis-b00fa02b0d4b\n\n추가적으로 Quick Start의 내용을 함께 포함하였습니다.  \nhttps://quickstarts.snowflake.com/guide/query-cost-monitoring/index.html?index=..%2F..index#0\nhttps://quickstarts.snowflake.com/guide/getting_started_cost_performance_optimization/index.html?index=..%2F..index#0\n\n\n_Python 코드가 원할하게 수행되기 위해서는 우측상단의 Packages에서 plotly, nbformat을 추가해야 합니다_"
  },
  {
   "cell_type": "markdown",
   "id": "ebdbf16a-a6af-4e00-b01d-320b068e6b17",
   "metadata": {
    "name": "Package_Period",
    "collapsed": false
   },
   "source": "# 패키지 설정 및 측정 날짜 지정"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Packages",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px \nimport datetime\nimport altair as alt\nimport plotly.graph_objects as go \nimport numpy as np\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark.window import Window\n\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "67ebca6d-7210-4860-9e17-3e5506501df8",
   "metadata": {
    "language": "python",
    "name": "DateFilter",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('모니터링 날짜 범위 지정')\n\nmax_date = datetime.datetime.now()\nmin_date = datetime.datetime.now() - datetime.timedelta(days=365)\nthis_year = max_date.year\njan_1 = datetime.date(this_year, 1, 1)\ndec_31 = datetime.date(this_year, 12, 31)\n\nif 'starting' not in st.session_state:\n    st.session_state.starting = datetime.datetime.now() - datetime.timedelta(days=30)\n\nif 'ending' not in st.session_state:\n    st.session_state.ending = max_date\n\nst.markdown(\"Enter your desired date range (30 days on initial load):\")\n\n#Column for Date Picker Buttons\ncol1, col2, col3, col4, col5 = st.columns([1,1,1,1,1])\n\nwith col1:\n    if st.button('7 Days'):\n            st.session_state.starting = datetime.datetime.now() - datetime.timedelta(days=7)\n            st.session_state.ending = datetime.datetime.now()\nwith col2:\n    if st.button('14 Days'):\n            st.session_state.starting = datetime.datetime.now() - datetime.timedelta(days=14)\n            st.session_state.ending = datetime.datetime.now()\nwith col3:\n    if st.button('30 Days'):\n            st.session_state.starting = datetime.datetime.now() - datetime.timedelta(days=30)\n            st.session_state.ending = datetime.datetime.now()\nwith col4:\n    if st.button('60 Days'):\n            st.session_state.starting = datetime.datetime.now() - datetime.timedelta(days=60)\n            st.session_state.ending = datetime.datetime.now()\nwith col5:\n    if st.button('120 Days'):\n            st.session_state.starting = datetime.datetime.now() - datetime.timedelta(days=120)\n            st.session_state.ending = datetime.datetime.now()\n\n#Date Input\ndate_input_filter = st.date_input(\n    \"\",\n    (st.session_state.starting,st.session_state.ending),\n    min_date,\n    max_date,\n)\n\n#Start and End Date (s = start, e = end)\ns,e = date_input_filter\n\nst.divider()\n\n# Get the current credentials\nsession = get_active_session()\ncredits_used_df = session.sql",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b28462c4-e36e-40ef-881a-a6d7870a3656",
   "metadata": {
    "name": "Overviews",
    "collapsed": false
   },
   "source": "# 사용량 개요"
  },
  {
   "cell_type": "code",
   "id": "fd14a646-e49d-4ca2-8f19-b10607236b02",
   "metadata": {
    "language": "sql",
    "name": "Credits_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Credits Used\nselect round(sum(credits_used),0) as total_credits \n  from snowflake.account_usage.metering_history \n where start_time between '{{s}}' and '{{e}}' ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9493812-576c-4cbc-bfbc-1bc7545fb55c",
   "metadata": {
    "language": "sql",
    "name": "Num_Jobs_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Total # of Jobs Executed\nselect count(*) as number_of_jobs \n  from snowflake.account_usage.query_history \n where start_time between '{{s}}' and '{{e}}' ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "558d46ad-1de9-463d-b79d-79a500845c3b",
   "metadata": {
    "language": "sql",
    "name": "Current_Storage_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Current Storage\nselect round(avg(storage_bytes + stage_bytes + failsafe_bytes) / power(1024, 4),2) as billable_tb \n  from snowflake.account_usage.storage_usage \n where USAGE_DATE = current_date() - 1;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1f7348a-f7ea-47d6-9683-606961991846",
   "metadata": {
    "language": "python",
    "name": "Cards",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('Account 사용량 개요')\n \npandas_credits_used_df = Credits_SQL.to_pandas()\npandas_num_jobs_df = Num_Jobs_SQL.to_pandas()\npandas_current_storage_df = Current_Storage_SQL.to_pandas()\n\n#Final Value\n#modified by JH, 2025-04-26 \ncredits_used_tile = pandas_credits_used_df.iloc[0].values[0]\nnum_jobs_tile = pandas_num_jobs_df.iloc[0].values[0]\ncurrent_storage_tile = pandas_current_storage_df.iloc[0].values[0]\n\n#Column formatting and metrics of header 3 metrics\ncol1, col2, col3 = st.columns(3)\ncol1.metric(\"Credits Used\",\"{:,}\".format(int(credits_used_tile))) \ncol2.metric(\"Total # of Jobs Executed\",\"{:,}\".format(int(num_jobs_tile))) \ncol3.metric(\"Current Storage (TB)\",current_storage_tile)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf2617f9-6e82-44e7-89c7-a17ef4e460b4",
   "metadata": {
    "language": "sql",
    "name": "Credits_by_Month_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Credits Billed by Month \nselect date_trunc('MONTH', usage_date) as Usage_Month, \n       sum(CREDITS_BILLED) as sum_credits\n  from snowflake.account_usage.metering_daily_history \n WHERE usage_date >= DATEADD('MONTH', -12, DATE_TRUNC('MONTH', CURRENT_TIMESTAMP()))\n group by Usage_Month ;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9889cca-c38f-4fe7-ae17-a48d588bc974",
   "metadata": {
    "language": "python",
    "name": "Credits_by_Month_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('월별 전체 크레딧 사용량 추이')\n\ncredits_billed_df = Credits_by_Month_SQL.to_pandas()\n#st.write(credits_billed_df)\nfig_credits_billed=px.bar(credits_billed_df,x='USAGE_MONTH',y='SUM_CREDITS', orientation='v',title=\"Credits Billed by Month\")\nst.plotly_chart(fig_credits_billed, use_container_width=True)\n\nst.info('The above chart is static and not modified by the date range filter', icon=\"ℹ️\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91337b88-9588-409e-9610-f50669ae8c30",
   "metadata": {
    "language": "sql",
    "name": "Job_by_Month_SQL"
   },
   "outputs": [],
   "source": "--Total # of Jobs Executed by Month \n--Added by JH, 2025-05-03\nSELECT DATE_TRUNC('MONTH', START_TIME) AS job_month, \n       COUNT(*) AS number_of_jobs                   \n  FROM snowflake.account_usage.query_history       \n WHERE START_TIME >= DATEADD('MONTH', -12, DATE_TRUNC('MONTH', CURRENT_TIMESTAMP()))        \n GROUP BY 1                                            \n ORDER BY 1 ASC                                        \n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38788de3-b4fe-4d57-b905-7b69279f91aa",
   "metadata": {
    "language": "python",
    "name": "Job_by_Month_Python"
   },
   "outputs": [],
   "source": "st.title('월별 전체 Job 수행 수 변화 추이')\n\nJobByMonth_df = Job_by_Month_SQL.to_pandas()\nfig_JobByMonth=px.bar(JobByMonth_df,x='JOB_MONTH',y='NUMBER_OF_JOBS', orientation='v',title=\"Jobs by Month\")\nst.plotly_chart(fig_JobByMonth, use_container_width=True)\n\nst.info('The above chart is static and not modified by the date range filter', icon=\"ℹ️\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "89ffa09e-b630-4a56-a455-75c127d46ba2",
   "metadata": {
    "name": "VirtualWarehouse",
    "collapsed": false
   },
   "source": "# 가상 웨어하우스 (Virtual Warehouse) 분석"
  },
  {
   "cell_type": "code",
   "id": "9fd3e633-3ef3-4076-8960-23da6a1effb6",
   "metadata": {
    "language": "sql",
    "name": "Credit_Usage_WH_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Credits Usages of Warehouse\nselect warehouse_name,sum(credits_used) as total_credits_used \n  from snowflake.account_usage.warehouse_metering_history \n where start_time between '{{s}}' and '{{e}}' \n group by 1 \n order by 2 desc \n limit 10 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a617b1d5-6731-4e8a-bec5-2285cd5cd501",
   "metadata": {
    "language": "python",
    "name": "Credit_Usage_WH_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간 웨어하우스별 사용된 크레딧')\n\n#Convert to a pandas df\npandas_credits_used_df = Credit_Usage_WH_SQL.to_pandas()\n\n#Added by JH, 2025-04-26 (for descending order)\npandas_credits_used_df_sorted = pandas_credits_used_df.sort_values(by='TOTAL_CREDITS_USED', ascending=True)\n\n#Chart\nfig_credits_used=px.bar(pandas_credits_used_df_sorted,x='TOTAL_CREDITS_USED',y='WAREHOUSE_NAME',orientation='h',title=\"Credits Used by Warehouse\")\nfig_credits_used.update_traces(marker_color='green')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f95ce49-caae-46e6-9568-f7605609900d",
   "metadata": {
    "language": "sql",
    "name": "Jobs_WH_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Jobs by Warehouse Data Setup\nselect warehouse_name,count(*) as number_of_jobs \n  from snowflake.account_usage.query_history \n where start_time between '{{s}}' and '{{e}}' \n   and warehouse_name is not null  --added by JH, 2025-04-26\n group by 1 \n order by 2 desc \n limit 10 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23a71cd4-cbff-43b3-809c-5ccc6915731c",
   "metadata": {
    "language": "python",
    "name": "Jobs_WH_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간 웨어하우스별 수행된 작업의 수')\n\n#convert to pandas df\npandas_jobs_by_warehouse_df = Jobs_WH_SQL.to_pandas()\n\n#Added by JH, 2025-04-26 (for descending order)\npandas_jobs_by_warehouse_df_sorted = pandas_jobs_by_warehouse_df.sort_values(by='NUMBER_OF_JOBS', ascending=True)\n\n#chart\nfig_jobs_by_warehouse=px.bar(pandas_jobs_by_warehouse_df_sorted,x='NUMBER_OF_JOBS',y='WAREHOUSE_NAME',orientation='h',title=\"# of Jobs by Warehouse\")\nfig_jobs_by_warehouse.update_traces(marker_color='purple')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e5d8656-ea97-45e1-b0b1-9a65c2e97a31",
   "metadata": {
    "language": "sql",
    "name": "QueryType_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Average Execution by Query Type\nselect query_type, warehouse_size, avg(execution_time) / 1000 as average_execution_time \n  from snowflake.account_usage.query_history \n where start_time between '{{s}}' and '{{e}}' \n group by 1, 2 \n order by 3 desc;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2dbb205-aea2-4430-83b2-bf2404e4ea69",
   "metadata": {
    "language": "python",
    "name": "QueryType_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간 쿼리 유형별, 웨어하우스 크기별 평균 실행 시간 (초)')\n\n#convert to pandas df\npandas_execution_by_qtype_df = QueryType_SQL.to_pandas()\n\n#Modified by JH, 2025-04-26 \npandas_execution_by_qtype_df=px.bar(pandas_execution_by_qtype_df,x='AVERAGE_EXECUTION_TIME',y='QUERY_TYPE',color='WAREHOUSE_SIZE',orientation='h',title=\"Average Execution Time by Query Type and Warehouse Size\")\nst.plotly_chart(pandas_execution_by_qtype_df, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f92e365-4f7f-4e4f-9ea2-6072be0eda08",
   "metadata": {
    "language": "sql",
    "name": "Credits_Used_Overtime_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Credits Used Overtime\nselect start_time::date as usage_date, warehouse_name, sum(credits_used) as total_credits_used \n  from snowflake.account_usage.warehouse_metering_history \n where start_time between '{{s}}' and '{{e}}' \n group by 1,2 \nhaving total_credits_used > 0.001 --Added by JH, 2025-04=26 \n order by 2,1 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c322f900-b6d0-41ac-8630-d660344d793a",
   "metadata": {
    "language": "python",
    "name": "Credits_Used_Overtime_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간 웨어하우스별 크레딧 사용양 추세')\n\n#convert to pandas df\npandas_credits_used_overtime_df = Credits_Used_Overtime_SQL.to_pandas()\n\n#chart\nfig_credits_used_overtime_df=px.bar(pandas_credits_used_overtime_df,x='USAGE_DATE',y='TOTAL_CREDITS_USED',color='WAREHOUSE_NAME',orientation='v',title=\"Credits Used Overtime\")\nst.plotly_chart(fig_credits_used_overtime_df, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38cc2f7b-370d-4d77-a512-1cf84963372d",
   "metadata": {
    "language": "sql",
    "name": "Warehouse_Variance_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Warehouse Variance Overtime\nSELECT WAREHOUSE_NAME, \n       DATE(START_TIME) AS DATE, \n       SUM(CREDITS_USED) AS CREDITS_USED, \n       AVG(SUM(CREDITS_USED)) OVER (PARTITION BY WAREHOUSE_NAME ORDER BY DATE ROWS 7 PRECEDING) \n         AS CREDITS_USED_7_DAY_AVG, \n       (TO_NUMERIC(SUM(CREDITS_USED)/CREDITS_USED_7_DAY_AVG*100,10,2)-100)::STRING || '%' \n         AS VARIANCE_TO_7_DAY_AVERAGE \n  FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY \n where start_time between '{{s}}' and '{{e}}' \n GROUP BY DATE, WAREHOUSE_NAME \n ORDER BY DATE DESC ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61be905e-a899-4b83-b74a-ee75c1214768",
   "metadata": {
    "language": "python",
    "name": "Warehouse_Variance_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간내의 해당 날짜별 이전 7일간의 평균 웨어하우스 사용량 대비 증감양')\n\n#convert to pandas\npandas_warehouse_variance_df = Warehouse_Variance_SQL.to_pandas()\n\n#chart\nfig_warehouse_variance_df=px.bar(pandas_warehouse_variance_df,x=\"DATE\",y=\"VARIANCE_TO_7_DAY_AVERAGE\",color ='WAREHOUSE_NAME',orientation='v',title=\"Warehouse Variance Greater than 7 day Average\")\nst.plotly_chart(fig_warehouse_variance_df, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5640d1d-a22d-466e-93ed-298a5aeb93a4",
   "metadata": {
    "language": "sql",
    "name": "Exe_Time_in_WH_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "with base_table as (\nselect warehouse_name, \n       total_elapsed_time,\n       case\n            when total_elapsed_time < 1 then 'a'\n            when total_elapsed_time < 10 then 'b'\n            when total_elapsed_time < 60 then 'c'\n            when total_elapsed_time < 300 then 'd'\n            when total_elapsed_time < 600 then 'e'\n            when total_elapsed_time > 600 then 'f'\n       end as exe_time\n  from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\n where warehouse_name is not null\n   and start_time between '{{s}}' and '{{e}}'\n   and warehouse_name not ilike 'compute_service%'   \n)\nselect warehouse_name, \n       sum(iff(exe_time= 'a', 1, 0)) u00_01s,\n       sum(iff(exe_time= 'b', 1, 0)) u01_10s,\n       sum(iff(exe_time= 'c', 1, 0)) u10_60s,\n       sum(iff(exe_time= 'd', 1, 0)) u01_05m,\n       sum(iff(exe_time= 'e', 1, 0)) u05_10m,\n       sum(iff(exe_time= 'f', 1, 0)) u10m_\n  from base_table\n group by warehouse_name;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff69eb89-c082-48e9-a30f-9b94d0b74714",
   "metadata": {
    "language": "python",
    "name": "Exe_Time_in_WH_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간 동안 웨어하우스별 워크로드 수행시간 분포')\n\nExeTimeInWH_df = Exe_Time_in_WH_SQL.to_pandas()\n\n# --- 데이터 준비 ---\nplot_columns = ['U00_01S', 'U01_10S', 'U10_60S', 'U01_05M', 'U05_10M', 'U10M_']\n\n# 레이블 매핑 (melt 후의 컬럼명 'variable', 'value' 포함)\nlabels_map = {\n    'WAREHOUSE_NAME': 'Warehouse Name',\n    'variable': 'Execution Time Bucket', # melt 후 범례가 될 컬럼\n    'value': 'Number of Queries',      # melt 후 y축 값이 될 컬럼\n    'U00_01S': '< 1s',                 # variable 컬럼의 값들에 대한 레이블\n    'U01_10S': '1s - 10s',\n    'U10_60S': '10s - 60s',\n    'U01_05M': '1m - 5m',\n    'U05_10M': '5m - 10m',\n    'U10M_': '> 10m'\n}\n\n# --- 데이터 변환 (Wide to Long) ---\n# id_vars: 고정할 컬럼 (x축)\n# value_vars: 행으로 변환할 컬럼들 (y축 값들)\n# var_name: value_vars의 이름들이 들어갈 새 컬럼명 (범례/색상 구분)\n# value_name: value_vars의 값들이 들어갈 새 컬럼명 (y축 값)\ndf_long = pd.melt(ExeTimeInWH_df,\n                  id_vars='WAREHOUSE_NAME',\n                  value_vars=plot_columns,\n                  var_name='variable',  # labels_map의 'variable' 키와 일치\n                  value_name='value')   # labels_map의 'value' 키와 일치\n\n# --- Plotly Express로 스택 바 차트 생성 (변환된 데이터 사용) ---\nfig = px.bar(\n    df_long,                   # 변환된 long-form 데이터 사용\n    x='WAREHOUSE_NAME',        # X축: Warehouse 이름 컬럼\n    y='value',                 # Y축: melt된 값 컬럼 ('Number of Queries')\n    color='variable',          # 색상 구분: melt된 변수 컬럼 ('Execution Time Bucket')\n    title='Query Execution Time Distribution by Warehouse',\n    labels=labels_map,         # 축 및 범례 레이블 매핑 적용\n                               # labels_map이 'variable'과 'value'를 포함하므로 자동 적용됨\n    height=500,\n    # category_orders를 사용하여 범례 순서 지정 (선택 사항)\n    category_orders={'variable': plot_columns} # 원래 컬럼 순서대로 범례 정렬\n)\n\n# --- 차트 레이아웃 커스터마이징 (선택 사항) ---\n# labels 인자에서 대부분 처리되었으므로 명시적 설정은 필요 없을 수 있음\n# fig.update_layout(\n#     xaxis_title='Warehouse Name',\n#     yaxis_title='Number of Queries',\n#     legend_title_text='Execution Time Bucket',\n# )\nfig.update_yaxes(tickformat=',') # Y축 값에 쉼표 추가\n\n# --- Streamlit에 차트 표시 ---\nst.subheader('Execution Time Distribution in Warehouse')\nst.plotly_chart(fig, use_container_width=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3734c4c8-d4aa-48f7-b6d6-8a10f09d9a31",
   "metadata": {
    "language": "sql",
    "name": "WH_Workload_PCT_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Added by MJ, 2025-04-30\nselect warehouse_name,\n       percentile_cont(0.25) within group(order by query_load_percent) as p25,\n       median(query_load_percent) as md,\n       percentile_cont(0.75) within group(order by query_load_percent) as p75,\n  from snowflake.account_usage.query_history\n where warehouse_name is not null\n   and query_load_percent is not null\n   and start_time between '{{s}}' and '{{e}}'\n   and warehouse_name not ilike 'compute_service%'\n group by 1\n order by 3 desc ; ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce0ac44e-bb9e-4bf4-b144-7ba71e68ee1a",
   "metadata": {
    "language": "python",
    "name": "WH_Workload_PCT_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간 웨어하우스별 워크로드 사용량 분포 (25%, 50%, 75%)')\n\nWHWorkloadPCT_df = WH_Workload_PCT_SQL.to_pandas()\n\n# Plotly 시각화를 위한 데이터 준비 (오류 막대 길이 계산)\n# error_y: 중앙값(MD)에서 위쪽으로 P75까지의 거리\n# error_y_minus: 중앙값(MD)에서 아래쪽으로 P25까지의 거리\nWHWorkloadPCT_df['error_y_upper'] = WHWorkloadPCT_df['P75'] - WHWorkloadPCT_df['MD']\nWHWorkloadPCT_df['error_y_lower'] = WHWorkloadPCT_df['MD']  - WHWorkloadPCT_df['P25']\n\nfig = px.scatter(\n    WHWorkloadPCT_df,\n    x='WAREHOUSE_NAME',\n    y='MD',\n    error_y='error_y_upper',      # y값(MD) 기준 +방향 오차 (P75까지)\n    error_y_minus='error_y_lower',# y값(MD) 기준 -방향 오차 (P25까지)\n    title='Warehouse Query Load Distribution (Median and P25-P75 Range)',\n    labels={                      # 축 및 호버 레이블 설정\n        'WAREHOUSE_NAME': 'Warehouse',\n        'MD': 'Median Query Load (%)',\n        'P25': 'P25 Load (%)', # 호버 정보용\n        'P75': 'P75 Load (%)'  # 호버 정보용\n    },\n    hover_data=['P25', 'MD', 'P75'] # 호버 시 표시될 추가 데이터 지정\n)\n\n# --- 차트 레이아웃 커스터마이징 ---\nfig.update_layout(\n    yaxis_title='Query Load Percent (%)', # Y축 제목 명확화\n    xaxis_title='Warehouse Name',\n    yaxis_range=[0, max(100, WHWorkloadPCT_df['P75'].max() * 1.1)], # Y축 범위 설정 (0 ~ 최대 P75값보다 조금 크게)\n    yaxis_ticksuffix='%' # Y축 값 뒤에 '%' 추가\n)\n\n# 오류 막대(Error Bars) 스타일 조정 (선택 사항)\nfig.update_traces(\n    error_y_thickness=1,   # 오류 막대 선 두께\n    error_y_width=5        # 오류 막대 상/하단 너비\n    # selector=dict(type='scatter') # 특정 trace만 선택할 경우\n)\n\n# --- Streamlit에 차트 표시 ---\nst.plotly_chart(fig, use_container_width=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f1b82ae-cc7e-47c7-ae70-1cd15c80307e",
   "metadata": {
    "language": "sql",
    "name": "Run_Queue_WH_SQL"
   },
   "outputs": [],
   "source": "--Added by Chanwoo Park, 2025-05-02\n\nSELECT to_date(start_time) as date\n      ,warehouse_name\n      ,sum(avg_running) as sum_running\n      ,sum(avg_queued_load) as sum_queued\n  FROM snowflake.account_usage.warehouse_load_history\n WHERE to_date(start_time) >= dateadd(month, -1, current_timestamp())\n GROUP BY 1,2\n ORDER BY 1,2 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9c6df7d-4ff4-4f08-b6df-2f1ad5e65b18",
   "metadata": {
    "language": "python",
    "name": "Run_Queue_WH_Python"
   },
   "outputs": [],
   "source": "# Added by JH, 2025-05-02\nst.title('일별 웨어하우스의 워크로드 추이 (지난 1개월)')\n\nRunQueueWH_df = Run_Queue_WH_SQL.to_pandas()\n\n# --- Plotly Express로 라인 차트 생성 ---\n# SUM_RUNNING 추이 그래프\nfig_running = px.line(\n    RunQueueWH_df,\n    x='DATE',\n    y='SUM_RUNNING',\n    color='WAREHOUSE_NAME',          # 웨어하우스별로 다른 색상 라인\n    title='Daily Sum of Average Running Queries by Warehouse',\n    labels={                         # 축 및 범례 레이블 설정\n        'DATE': 'Date',\n        'SUM_RUNNING': 'Sum of Avg Running Queries (Daily)',\n        'WAREHOUSE_NAME': 'Warehouse'\n    },\n    markers=True,                   # 각 데이터 포인트에 마커 표시 (선택 사항)\n    hover_data={'WAREHOUSE_NAME': True, 'SUM_RUNNING': ':.1f'} # 호버 데이터 형식 지정\n)\nfig_running.update_layout(\n    xaxis_title='Date',\n    yaxis_title='Daily Sum of Avg Running Queries',\n    hovermode='x unified' # 같은 날짜의 모든 웨어하우스 정보 함께 표시\n)\nst.plotly_chart(fig_running, use_container_width=True)\n\n# SUM_QUEUED 추이 그래프\nfig_queued = px.line(\n    RunQueueWH_df,\n    x='DATE',\n    y='SUM_QUEUED',\n    color='WAREHOUSE_NAME',          # 웨어하우스별로 다른 색상 라인\n    title='Daily Sum of Average Queued Queries by Warehouse',\n    labels={                         # 축 및 범례 레이블 설정\n        'DATE': 'Date',\n        'SUM_QUEUED': 'Sum of Avg Queued Queries (Daily)',\n        'WAREHOUSE_NAME': 'Warehouse'\n    },\n    markers=True,                   # 각 데이터 포인트에 마커 표시 (선택 사항)\n    hover_data={'WAREHOUSE_NAME': True, 'SUM_QUEUED': ':.1f'} # 호버 데이터 형식 지정\n)\nfig_queued.update_layout(\n    xaxis_title='Date',\n    yaxis_title='Daily Sum of Avg Queued Queries',\n    hovermode='x unified' # 같은 날짜의 모든 웨어하우스 정보 함께 표시\n)\n# Queued 값이 0인 경우가 많으므로 Y축 범위를 동적으로 설정하거나, 0 이상으로만 설정\nmin_queued = RunQueueWH_df['SUM_QUEUED'].min()\nmax_queued = RunQueueWH_df['SUM_QUEUED'].max()\n# 0으로만 구성된 경우 max_queued가 0이 되어 range가 [0,0]이 되는것 방지\nfig_queued.update_yaxes(range=[min_queued, max(1, max_queued * 1.1)]) # 최소 1 이상의 범위를 가지도록 함\n\nst.plotly_chart(fig_queued, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0119a064-12cb-4584-9b85-d4b167d44415",
   "metadata": {
    "name": "Queries",
    "collapsed": false
   },
   "source": "# 쿼리 분석"
  },
  {
   "cell_type": "code",
   "id": "623daba8-1d7b-422a-9391-6cc7258ed05c",
   "metadata": {
    "language": "sql",
    "name": "Longest_Query_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Top 25 Longest Success Queries\nselect query_id,query_text,(execution_time / 1000) as exec_time \n  from snowflake.account_usage.query_history \n where execution_status = 'SUCCESS' \n   and start_time between '{{s}}' and '{{e}}' \n   and query_text != '' --Added by JH, 2025-04-26 \n   and exec_time > 10   --Added by JH, 2025-04-26\n order by execution_time desc \n limit 25;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d239d61-cbe2-4e1b-8ece-eb80f99a03c0",
   "metadata": {
    "language": "python",
    "name": "Longest_Query_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간동안 가장 오래 수행된 쿼리 리스트')\n\n#convert to pandas df\npandas_longest_queries_df = Longest_Query_SQL.to_pandas()\n\n#Modifed by JH, 2025-04-26\nmax_len = 100\ncol_orig = 'QUERY_TEXT'           # 원본 텍스트 컬럼명\ncol_short = 'QUERY_TEXT_SHORT'    # 새로 만들 짧은 텍스트 컬럼명\n\n# 지정된 길이로 텍스트 자르기\npandas_longest_queries_df[col_short] = pandas_longest_queries_df[col_orig].str.slice(0, max_len)\n# 원본 길이가 max_len보다 긴 경우에만 끝에 '...' 추가\npandas_longest_queries_df.loc[pandas_longest_queries_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_longest_queries = px.bar(\n    pandas_longest_queries_df,\n    x='EXEC_TIME',\n    y=col_short,          # Y축에는 새로 만든 짧은 텍스트 컬럼 사용\n    orientation='h',\n    title=\"Longest Successful Queries (Top 25)\",\n    hover_data={          # 마우스 호버 시 표시될 정보 설정\n        'EXEC_TIME': ':.2f s', # 실행 시간 (소수점 2자리 초 단위 포맷)\n        col_orig: True,        # 원본 전체 QUERY_TEXT 표시\n        col_short: False       # Y축 레이블과 동일한 짧은 텍스트는 호버에서 숨김 (선택 사항)\n    },\n    labels={col_short: \"Query Text (Truncated)\"} # Y축 이름 변경 (선택 사항)\n)\n\nst.write(fig_longest_queries)\n\n#chart\n#fig_longest_queries=px.bar(pandas_longest_queries_df,x='EXEC_TIME',y='QUERY_TEXT',orientation='h',title=\"Longest Successful Queries (Top 25) \")\n#st.write(fig_longest_queries)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f571c218-fbca-4b1a-b08b-4d9d28a836c2",
   "metadata": {
    "language": "sql",
    "name": "Failed_Query_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Query Modified by JH, 2025-04-26\n--Top 25 Failed Queries \nselect query_text,count(*) as number_of_execution  \n  from snowflake.account_usage.query_history \n where execution_status = 'FAIL' \n   and start_time between '{{s}}' and '{{e}}' \n   and query_text != '' --Added by JH, 2025-04-26 \n group by 1\nhaving number_of_execution > 1\n order by 2 desc \n limit 25;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9237249f-78f9-4ca3-bbd0-5e0985fa9cb2",
   "metadata": {
    "language": "python",
    "name": "Failed_Query_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간동안 가장 많이 실패한 쿼리 문 리스트')\n\n#convert to pandas df\nf_pandas_longest_queries_df = Failed_Query_SQL.to_pandas()\n\n#Modifed by JH, 2025-04-26\nmax_len = 100\ncol_orig = 'QUERY_TEXT'           # 원본 텍스트 컬럼명\ncol_short = 'QUERY_TEXT_SHORT'    # 새로 만들 짧은 텍스트 컬럼명\n\n# 지정된 길이로 텍스트 자르기\nf_pandas_longest_queries_df[col_short] = f_pandas_longest_queries_df[col_orig].str.slice(0, max_len)\n# 원본 길이가 max_len보다 긴 경우에만 끝에 '...' 추가\nf_pandas_longest_queries_df.loc[f_pandas_longest_queries_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_f_longest_queries = px.bar(\n    f_pandas_longest_queries_df,\n    x='NUMBER_OF_EXECUTION',\n    y=col_short,          # Y축에는 새로 만든 짧은 텍스트 컬럼 사용\n    orientation='h',\n    title=\"Most Failed Queries (Top 25)\",\n    hover_data={          # 마우스 호버 시 표시될 정보 설정\n        'NUMBER_OF_EXECUTION': ':3d s', \n        col_orig: True,        # 원본 전체 QUERY_TEXT 표시\n        col_short: False       # Y축 레이블과 동일한 짧은 텍스트는 호버에서 숨김 (선택 사항)\n    },\n    labels={col_short: \"Query Text (Truncated)\"} # Y축 이름 변경 (선택 사항)\n)\nfig_f_longest_queries.update_traces(marker_color='red')\n\nst.write(fig_f_longest_queries)\n\n#chart\n#fig_f_longest_queries=px.bar(f_pandas_longest_queries_df,x='EXEC_TIME',y='QUERY_TEXT',orientation='h',title=\"Longest Failed Queries (Top 25)\")\n#fig_f_longest_queries.update_traces(marker_color='red')\n#st.write(fig_f_longest_queries)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68c6ff2f-fcbb-48c2-9bb4-91af0748a6b3",
   "metadata": {
    "language": "sql",
    "name": "Repeated_Query_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Total Execution Time by Repeated Queries\nselect query_text, \n       (sum(execution_time) / 1000) as exec_time,\n       count(*) as number_of_query\n  from snowflake.account_usage.query_history \n where execution_status = 'SUCCESS' \n   and start_time between '{{s}}' and '{{e}}' \n   and query_text != '' -- Added by JH, 2025-04-26\n group by query_text \n order by exec_time desc \n limit 10 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7d89c41-241b-4218-8e35-1a26a51a4573",
   "metadata": {
    "language": "python",
    "name": "Repeated_Query_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간내의 반복되는 쿼리 수행')\n\n#fig_execution_time=px.bar(Repeated_Query,x='EXEC_TIME',y='QUERY_TEXT', orientation='h',title=\"Total Execution Time by Repeated Queries\")\n#fig_execution_time.update_traces(marker_color='LightSkyBlue')\n#st.plotly_chart(fig_execution_time, use_container_width=True)\n\n#Modifed by JH, 2025-04-26\n\n#convert to pandas df\nTotal_Execution_Time_df = Repeated_Query_SQL.to_pandas()\n\nmax_len = 100\ncol_orig = 'QUERY_TEXT'           # 원본 텍스트 컬럼명\ncol_short = 'QUERY_TEXT_SHORT'    # 새로 만들 짧은 텍스트 컬럼명\n\n# 지정된 길이로 텍스트 자르기\nTotal_Execution_Time_df[col_short] = Total_Execution_Time_df[col_orig].str.slice(0, max_len)\n# 원본 길이가 max_len보다 긴 경우에만 끝에 '...' 추가\nTotal_Execution_Time_df.loc[Total_Execution_Time_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_Total_Execution_Time = px.bar(\n    Total_Execution_Time_df,\n    x='EXEC_TIME',\n    y=col_short,          # Y축에는 새로 만든 짧은 텍스트 컬럼 사용\n    orientation='h',\n    title=\"Total Execution Time by Repeated Queries\",\n    hover_data={          # 마우스 호버 시 표시될 정보 설정\n        'EXEC_TIME': ':.3f s',  # 실행 시간 (소수점 3자리 초 단위 포맷으로 수정)\n        col_orig: True,         # 원본 전체 QUERY_TEXT 표시\n        col_short: False,       # Y축 레이블과 동일한 짧은 텍스트는 호버에서 숨김 (선택 사항)\n        'NUMBER_OF_QUERY': ':,d' # <<<--- 추가: 쿼리 실행 횟수 (정수형, 콤마 구분 포맷)\n    },\n    labels={               # 축 및 호버 레이블 이름 설정 (선택 사항)\n        col_short: \"Query Text (Truncated)\",\n        'EXEC_TIME': \"Total Execution Time (s)\",\n        'NUMBER_OF_QUERY': \"Number of Executions\"\n    }\n)\nfig_Total_Execution_Time.update_traces(marker_color='LightSkyBlue')\n\nst.write(fig_Total_Execution_Time)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e4f1ac0-205b-4623-a599-47fbead7ca12",
   "metadata": {
    "language": "sql",
    "name": "Spill_to_Disk_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--QUERIES THAT SPILL TO DISK\nselect top 25\n       --hash(query_text) as query_hash\n       query_text\n      --,count(query_id) as query_count\n      ,avg(total_elapsed_time) as avg_total_elapsed_time\n      ,sum(bytes_spilled_to_local_storage) / 1024 / 1024 / 1024 as sum_gb_spilled_to_disk\n      ,sum(bytes_spilled_to_remote_storage) / 1024 / 1024 / 1024 as sum_gb_spilled_to_blob\n      ,sum(bytes_spilled_to_local_storage+bytes_spilled_to_remote_storage) as total_bytes\n  from snowflake.account_usage.query_history\n where start_time between '{{s}}' and '{{e}}' \n   and query_text != ''\n group by query_text\n--group by query_hash, query_text\n--having query_count > 5\nhaving total_bytes > 0\n order by total_bytes desc;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7d75793-785f-451e-8965-9d3789404334",
   "metadata": {
    "language": "python",
    "name": "Spill_to_Disk_Python"
   },
   "outputs": [],
   "source": "st.title('지정된 기간 동안 Disk로 Spill된 쿼리 리스트')\n\n#fig_execution_time=px.bar(Repeated_Query,x='EXEC_TIME',y='QUERY_TEXT', orientation='h',title=\"Total Execution Time by Repeated Queries\")\n#fig_execution_time.update_traces(marker_color='LightSkyBlue')\n#st.plotly_chart(fig_execution_time, use_container_width=True)\n\n#Modifed by JH, 2025-04-26\n\n#convert to pandas df\nSpillToDisk_df = Spill_to_Disk_SQL.to_pandas()\n\nmax_len = 100\ncol_orig = 'QUERY_TEXT'           # 원본 텍스트 컬럼명\ncol_short = 'QUERY_TEXT_SHORT'    # 새로 만들 짧은 텍스트 컬럼명\n\n# 지정된 길이로 텍스트 자르기\nSpillToDisk_df[col_short] = SpillToDisk_df[col_orig].str.slice(0, max_len)\n# 원본 길이가 max_len보다 긴 경우에만 끝에 '...' 추가\nSpillToDisk_df.loc[SpillToDisk_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_SpillToDisk = px.bar(\n    SpillToDisk_df,\n    x='TOTAL_BYTES',\n    y=col_short,          # Y축에는 새로 만든 짧은 텍스트 컬럼 사용\n    orientation='h',\n    title=\"Spill to Disk Queries\",\n    hover_data={          # 마우스 호버 시 표시될 정보 설정\n        'AVG_TOTAL_ELAPSED_TIME': ':.2f',  \n        'TOTAL_BYTES': ':.d',  \n        col_orig: True,         # 원본 전체 QUERY_TEXT 표시\n        col_short: False     # Y축 레이블과 동일한 짧은 텍스트는 호버에서 숨김 (선택 사항)\n    },\n    labels={               # 축 및 호버 레이블 이름 설정 (선택 사항)\n        col_short: \"Query Text (Truncated)\",\n        'TOTAL_BYTES': \"Spill to Disk\",\n        'AVG_TOTAL_ELAPSED_TIME': \"Average Elapsed Time\"\n    }\n)\nfig_SpillToDisk.update_traces(marker_color='Yellow')\n\nst.write(fig_SpillToDisk)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c763c32c-c380-44b7-83fd-2fc0abd923a6",
   "metadata": {
    "language": "sql",
    "name": "Query_Execution_By_User_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Top 10 Average Query Execution Time (By User)\nselect user_name, (avg(execution_time)) / 1000 as average_execution_time \n  from snowflake.account_usage.query_history \n where execution_status = 'SUCCESS'            --Added by JH, 2025-04-26\n   and start_time between '{{s}}' and '{{e}}'  --Added by JH, 2025-04-26\n group by 1 \n order by 2 desc limit 10 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2be499d-83f9-4020-a91e-e2d50a1cc27a",
   "metadata": {
    "language": "python",
    "name": "Query_Execution_By_User_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간동안 사용자별 평균 쿼리 실행 시간 (초)')\n\nquery_execution_df = Query_Execution_By_User_SQL.to_pandas()\n#st.write(query_execution_df)\nfig_cquery_execution=px.bar(query_execution_df,x='USER_NAME',y='AVERAGE_EXECUTION_TIME', orientation='v',title=\"Average Execution Time per User\")\nfig_cquery_execution.update_traces(marker_color='MediumPurple')\nst.plotly_chart(fig_cquery_execution,use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b3b03b3-5de1-40cb-b232-1048710470c2",
   "metadata": {
    "language": "sql",
    "name": "Query_Heatmap_1W_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT query_history.query_id,\n       query_history.query_text,\n       query_history.start_time,\n       query_history.end_time,\n       query_history.user_name,\n       query_history.database_name,\n       query_history.schema_name,\n       query_history.warehouse_name,\n       query_history.warehouse_size,\n       metering_history.credits_used,\n       execution_time/1000 as execution_time_s\n  FROM snowflake.account_usage.query_history\n  JOIN snowflake.account_usage.metering_history \n    ON query_history.start_time >= metering_history.start_time\n   AND query_history.end_time <= metering_history.end_time\n WHERE query_history.start_time >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())\n ORDER BY query_history.query_id;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a1e20a1-9e01-4623-a56c-fe9022654f0e",
   "metadata": {
    "language": "python",
    "name": "Query_Heatmap_1W_1_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('한 주간 시간대별 쿼리 사용량 #1 - Heatmap')\n\n#import pandas as pd\n#import streamlit as st\n##import altair as alt\n\n# Get data\ndf = Query_Heatmap_1W_SQL.to_pandas()\n\n# Create date filter slider\nst.subheader(\"Select time duration\")\n\ncol = st.columns(3)\n\nwith col[0]:\n    days = st.slider('Select number of days to analyze', \n                     min_value=1, \n                     max_value=7, \n                     value=7, \n                     step=1)\nwith col[1]:\n    var = st.selectbox(\"Select a variable\", ['WAREHOUSE_NAME', 'USER_NAME', 'WAREHOUSE_SIZE'])\nwith col[2]:\n    metric = st.selectbox(\"Select a metric\", [\"COUNT\", \"TOTAL_CREDITS_USED\"])\n\n# Filter data according to day duration\ndf['START_TIME'] = pd.to_datetime(df['START_TIME'])\nlatest_date = df['START_TIME'].max()\ncutoff_date = latest_date - pd.Timedelta(days=days)\nfiltered_df = df[df['START_TIME'] > cutoff_date].copy()\n    \n# Prepare data for heatmap\nfiltered_df['HOUR_OF_DAY'] = filtered_df['START_TIME'].dt.hour\nfiltered_df['HOUR_DISPLAY'] = filtered_df['HOUR_OF_DAY'].apply(lambda x: f\"{x:02d}:00\")\n    \n# Calculate frequency count and sum of credits by hour and query\nagg_df = (filtered_df.groupby(['QUERY_ID', 'HOUR_DISPLAY', var])\n          .agg(\n              COUNT=('QUERY_ID', 'size'),\n              TOTAL_CREDITS_USED=('CREDITS_USED', 'sum')\n          )\n          .reset_index()\n)\n\nst.warning(f\"Analyzing {var} data for the last {days} days!\")\n\n## Heatmap\nheatmap = alt.Chart(agg_df).mark_rect(stroke='black', strokeWidth=1).encode(\n    x='HOUR_DISPLAY:O',\n    y=alt.Y(f'{var}:N', \n            title='',\n            axis=alt.Axis(\n                labels=True,\n                labelLimit=250,\n                tickMinStep=1,\n                labelOverlap=False,\n                labelPadding=10\n            )),\n    color=f'{metric}:Q',\n    tooltip=['HOUR_DISPLAY', var, metric]\n).properties(\n    title=f'Query Activity Heatmap by Hour and {var}'\n)\n\nst.altair_chart(heatmap, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0f6a0bd-15da-4451-bd5a-9f9ab40e0a7c",
   "metadata": {
    "language": "python",
    "name": "Query_Heatmap_1W_2_Python"
   },
   "outputs": [],
   "source": "st.title('한 주간 시간대별 쿼리 사용량 #2 Stacked bar chart')\n\n#import pandas as pd\n#import streamlit as st\n##import altair as alt\n\n# Get data\ndf = Query_Heatmap_1W_SQL.to_pandas()\n\n# Create date filter slider\nst.subheader(\"Select time duration\")\n\ncol = st.columns(3)\n\nwith col[0]:\n    days = st.slider('Select number of days to analyze', \n                     min_value=1, \n                     max_value=7, \n                     value=7, \n                     step=1,\n                     key='days_slider_2')\nwith col[1]:\n    var = st.selectbox(\"Select a variable\", ['WAREHOUSE_NAME', 'USER_NAME', 'WAREHOUSE_SIZE'],\n                       key='variable_selectbox_2')  # 고유 키 추가\nwith col[2]:\n    metric = st.selectbox(\"Select a metric\", [\"COUNT\", \"TOTAL_CREDITS_USED\"],\n                         key='metric_selectbox_2')  # 고유 키 추가\n\n# Filter data according to day duration\ndf['START_TIME'] = pd.to_datetime(df['START_TIME'])\nlatest_date = df['START_TIME'].max()\ncutoff_date = latest_date - pd.Timedelta(days=days)\nfiltered_df = df[df['START_TIME'] > cutoff_date].copy()\n    \n# Prepare data for heatmap\nfiltered_df['HOUR_OF_DAY'] = filtered_df['START_TIME'].dt.hour\nfiltered_df['HOUR_DISPLAY'] = filtered_df['HOUR_OF_DAY'].apply(lambda x: f\"{x:02d}:00\")\n    \n# Calculate frequency count and sum of credits by hour and query\nagg_df = (filtered_df.groupby(['QUERY_ID', 'HOUR_DISPLAY', var])\n          .agg(\n              COUNT=('QUERY_ID', 'size'),\n              TOTAL_CREDITS_USED=('CREDITS_USED', 'sum')\n          )\n          .reset_index()\n)\n\nst.warning(f\"Analyzing {var} data for the last {days} days!\")\n\n## Stacked bar chart with time series\nbar_time = alt.Chart(agg_df).mark_bar().encode(\n    x='HOUR_DISPLAY:O',\n    y=f'{metric}:Q',\n    color=alt.Color(f'{var}:N', legend=alt.Legend(orient='bottom')),\n    tooltip=['HOUR_DISPLAY', var, metric]\n).properties(\n    title=f'Query Activity by Hour and {var}',\n    height=400\n)\n\nst.altair_chart(bar_time, use_container_width=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db908de2-cd1a-4449-8246-9da06e97d701",
   "metadata": {
    "language": "python",
    "name": "Query_Heatmap_1W_3_Python"
   },
   "outputs": [],
   "source": "st.title('한 주간 시간대별 쿼리 사용량 #3 - Bubble plot')\n\n#import pandas as pd\n#import streamlit as st\n##import altair as alt\n\n# Get data\ndf = Query_Heatmap_1W_SQL.to_pandas()\n\n# Create date filter slider\nst.subheader(\"Select time duration\")\n\ncol = st.columns(3)\n\nwith col[0]:\n    days = st.slider('Select number of days to analyze', \n                     min_value=1, \n                     max_value=7, \n                     value=7, \n                     step=1,\n                     key='days_slider_3')\nwith col[1]:\n    var = st.selectbox(\"Select a variable\", ['WAREHOUSE_NAME', 'USER_NAME', 'WAREHOUSE_SIZE'],\n                       key='variable_selectbox_3')\nwith col[2]:\n    metric = st.selectbox(\"Select a metric\", [\"COUNT\", \"TOTAL_CREDITS_USED\"],\n                         key='metric_selectbox_3')\n\n# Filter data according to day duration\ndf['START_TIME'] = pd.to_datetime(df['START_TIME'])\nlatest_date = df['START_TIME'].max()\ncutoff_date = latest_date - pd.Timedelta(days=days)\nfiltered_df = df[df['START_TIME'] > cutoff_date].copy()\n    \n# Prepare data for heatmap\nfiltered_df['HOUR_OF_DAY'] = filtered_df['START_TIME'].dt.hour\nfiltered_df['HOUR_DISPLAY'] = filtered_df['HOUR_OF_DAY'].apply(lambda x: f\"{x:02d}:00\")\n    \n# Calculate frequency count and sum of credits by hour and query\nagg_df = (filtered_df.groupby(['QUERY_ID', 'HOUR_DISPLAY', var])\n          .agg(\n              COUNT=('QUERY_ID', 'size'),\n              TOTAL_CREDITS_USED=('CREDITS_USED', 'sum')\n          )\n          .reset_index()\n)\n\nst.warning(f\"Analyzing {var} data for the last {days} days!\")\n\n## Bubble plot with size representing the metric\nbubble = alt.Chart(agg_df).mark_circle().encode(\n    x='HOUR_DISPLAY:O',\n    y=alt.Y(f'{var}:N', title=''),\n    size=alt.Size(f'{metric}:Q', legend=alt.Legend(title='Query Count')),\n    color=alt.Color(f'{var}:N', legend=None),\n    tooltip=['HOUR_DISPLAY', var, metric]\n).properties(\n    title=f'Query Distribution by Hour and {var}',\n    height=550\n)\n\nst.altair_chart(bubble, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c786aab2-1943-4cef-8480-874badeb86ef",
   "metadata": {
    "name": "CloudService",
    "collapsed": false
   },
   "source": "# 클라우드 서비스 영역"
  },
  {
   "cell_type": "code",
   "id": "b8fa6568-ae54-42af-b6e9-e6d1b3ccda30",
   "metadata": {
    "language": "sql",
    "name": "GS_Util_By_Query_Type_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--GS Utilization by Query Type (Top 10)\nselect query_type, \n       sum(credits_used_cloud_services) cs_credits, \n       count(1) num_queries \n  from snowflake.account_usage.query_history \n where true \n   and start_time between '{{s}}' and '{{e}}'  --Added by JH, 2025-04-26\n group by 1 \n order by 2 desc \n limit 10 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d92ddf0-709b-4584-aea4-35045b128701",
   "metadata": {
    "language": "python",
    "name": "GS_Util_By_Query_Type_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간동안 쿼리 유형별 클라우드서비스 총 사용 크레딧량')\n\ngs_utilization_df = GS_Util_By_Query_Type_SQL.to_pandas()\n#fig_gs_utilization=px.bar(gs_utilization_df,x='QUERY_TYPE',y='CS_CREDITS', orientation='v',title=\"GS Utilization by Query Type (Top 10)\")\n\n#Added by JH, 2025-04-27\nfig_gs_utilization = px.bar(\n    gs_utilization_df,\n    x='QUERY_TYPE',\n    y='CS_CREDITS',\n    orientation='v', # Vertical orientation\n    title=\"GS Utilization by Query Type (Top 10)\",\n    hover_data={ # Define data to show on hover\n        'QUERY_TYPE': False, # Hide QUERY_TYPE in hover (already on x-axis)\n        'CS_CREDITS': ':.2f credits', # Format CS_CREDITS (e.g., 2 decimal places)\n        'NUM_QUERIES': ':,d' # Add NUM_QUERIES, format as integer with commas\n    },\n    labels={ # Customize labels shown in hover and on axes\n        'CS_CREDITS': 'Cloud Service Credits',\n        'NUM_QUERIES': 'Number of Queries'\n    }\n)\nfig_gs_utilization.update_traces(marker_color='green')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e33ae16a-2e86-4ea6-abf8-56d467639bbf",
   "metadata": {
    "language": "sql",
    "name": "Cloud_Service_By_Warehouse_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Top 10 Cloud Services by Warehouse   \nselect warehouse_name, \n       sum(credits_used_cloud_services) CREDITS_USED_CLOUD_SERVICES \n  from snowflake.account_usage.warehouse_metering_history \n where true \n   and start_time between '{{s}}' and '{{e}}'  --Added by JH, 2025-04-26\n group by 1 \n order by 2 desc limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9ebbc54-4937-4554-8031-843055ac9a87",
   "metadata": {
    "language": "python",
    "name": "Cloud_Service_By_Warehouse_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간동안 웨어하우스별 클라우드서비스의 총 사용 크레딧양')\n\ncompute_gs_by_warehouse_df = Cloud_Service_By_Warehouse_SQL.to_pandas()\n#st.write(compute_gs_by_warehouse_df)\nfig_compute_gs_by_warehouse=px.bar(compute_gs_by_warehouse_df,x='WAREHOUSE_NAME',y='CREDITS_USED_CLOUD_SERVICES', orientation='v',title=\"Compute and Cloud Services by Warehouse\", barmode=\"group\")\nfig_compute_gs_by_warehouse.update_traces(marker_color='purple')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba394452-2b8b-4fd3-8e13-15e756faeb4f",
   "metadata": {
    "name": "Storage",
    "collapsed": false
   },
   "source": "# 스토리지"
  },
  {
   "cell_type": "code",
   "id": "ebadc6bc-8917-49f3-a252-359c91f04b66",
   "metadata": {
    "language": "sql",
    "name": "Storage_Overtime_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Data Storage used Overtime \nselect date_trunc(month, usage_date) as usage_month, \n       avg(storage_bytes + stage_bytes + failsafe_bytes) / power(1024, 4) as billable_tb, \n       avg(storage_bytes) / power(1024, 4) as Storage_TB, \n       avg(stage_bytes) / power(1024, 4) as Stage_TB, \n       avg(failsafe_bytes) / power(1024, 4) as Failsafe_TB \n  from snowflake.account_usage.storage_usage \n group by 1 \n order by 1 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ba1293e-758d-4ecf-bccd-564ef18f43fc",
   "metadata": {
    "language": "python",
    "name": "Storage_Overtime_SQL_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('월별 스토리지 사용량 추이')\n\n# storage_overtime_df = Storage_Overtime_SQL.to_pandas()\n\n# fig_storage_overtime=px.bar(storage_overtime_df,x='USAGE_MONTH',y='BILLABLE_TB', orientation='v',title=\"Data Storage used Overtime\", barmode=\"group\")\n# st.plotly_chart(fig_storage_overtime, use_container_width=True)\n\n# st.info('The above chart is static and non modified by the date range filter', icon=\"ℹ️\")\n\n# Modified by JH, 2025-04-27\nstorage_overtime_df = Storage_Overtime_SQL.to_pandas()\n\n# --- 데이터 변환 (Wide to Long) ---\nstorage_df_melted = pd.melt(\n    storage_overtime_df,\n    id_vars=['USAGE_MONTH'], # 기준이 되는 컬럼\n    value_vars=['STORAGE_TB', 'STAGE_TB', 'FAILSAFE_TB'], # 누적할 값 컬럼들\n    var_name='STORAGE_TYPE', # 스토리지 유형을 나타낼 새 컬럼 이름\n    value_name='SIZE_TB'      # 스토리지 크기 값을 저장할 새 컬럼 이름\n)\n\n# (선택 사항) 스토리지 유형 순서 지정 (예: Storage -> Stage -> Failsafe)\nstorage_type_order = ['STORAGE_TB', 'STAGE_TB', 'FAILSAFE_TB']\nstorage_df_melted['STORAGE_TYPE'] = pd.Categorical(\n    storage_df_melted['STORAGE_TYPE'], categories=storage_type_order, ordered=True\n)\nstorage_df_melted = storage_df_melted.sort_values(by=['USAGE_MONTH', 'STORAGE_TYPE'])\n\n\n# --- 누적 막대 차트 생성 ---\nfig_stacked_storage = px.bar(\n    storage_df_melted,\n    x='USAGE_MONTH',    # x축: 월 (또는 다른 기준 컬럼)\n    y='SIZE_TB',          # y축: 스토리지 크기\n    color='STORAGE_TYPE', # 이 컬럼 기준으로 막대를 누적하고 색상 구분\n    title=\"Data Storage used Overtime\", # 차트 제목\n    orientation='v',      # 세로 막대 차트\n    hover_data={          # 마우스 호버 시 표시될 정보 설정\n        'USAGE_MONTH': True,\n        'STORAGE_TYPE': True,\n        'SIZE_TB': ':.2f TB' # 스토리지 크기 (소수점 2자리 TB 단위 포맷)\n    },\n    labels={              # 축 및 범례/호버 레이블 이름 설정\n        'USAGE_MONTH': 'Month',\n        'SIZE_TB': 'Storage Size (TB)',\n        'STORAGE_TYPE': 'Storage Type'\n    },\n    # color_discrete_map={ # 특정 색상 지정 (선택 사항)\n    #     'STORAGE_TB': 'blue',\n    #     'STAGE_TB': 'orange',\n    #     'FAILSAFE_TB': 'red'\n    # }\n)\n\nst.plotly_chart(fig_stacked_storage) # Streamlit 환경에서 사용\n\nst.info('The above chart is static and non modified by the date range filter', icon=\"ℹ️\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3b8846d-dc13-455c-8305-ee883e049d92",
   "metadata": {
    "language": "sql",
    "name": "Rows_Loaded_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Rows Loaded Overtime (COPY INTO) \nselect to_timestamp(date_trunc(day,last_load_time)) as usage_date, \n       sum(row_count) as total_rows \n  from snowflake.account_usage.load_history \n where usage_date between '{{s}}' and '{{e}}' \n group by 1 \n order by usage_date desc ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5adbcbc-01c3-4253-9edc-a2fd40ab3e4e",
   "metadata": {
    "language": "python",
    "name": "Rows_Loaded_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간동안 일별 데이터 적재건수 추이')\n\nrows_loaded_df = Rows_Loaded_SQL.to_pandas()\n\nfig_rows_loaded=px.bar(rows_loaded_df,x='USAGE_DATE',y='TOTAL_ROWS', orientation='v',title=\"Rows Loaded Overtime (Copy Into)\")\nst.plotly_chart(fig_rows_loaded, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2e7a739-b3e1-4799-b031-07fe94a4c29b",
   "metadata": {
    "name": "Users",
    "collapsed": false
   },
   "source": "# 사용자"
  },
  {
   "cell_type": "code",
   "id": "10ae6f79-2317-493d-b296-1625318fbb41",
   "metadata": {
    "language": "sql",
    "name": "Login_User_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Logins by User \nselect user_name, \n       sum(iff(is_success = 'NO', 1, 0)) as Failed, \n       count(*) as Success, \n       sum(iff(is_success = 'NO', 1, 0)) / nullif(count(*), 0) as login_failure_rate \n  from snowflake.account_usage.login_history \n where true\n   and event_timestamp between '{{s}}' and '{{e}}' --Added by JH, 2025-04-27\n group by 1 \n order by 4 desc;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cfce8caa-ce7b-4f20-89dd-099ea0766a89",
   "metadata": {
    "language": "python",
    "name": "Login_User_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간동안 사용자별 로그인 성공/실패 횟수')\n\nlogins_df_wide = Login_User_SQL.to_pandas()\n\n#Modified by JH, 2025-04-27\nlogins_df_melted = pd.melt(\n    logins_df_wide,\n    id_vars=['USER_NAME'],              # 기준이 되는 컬럼\n    value_vars=['SUCCESS', 'FAILED'],   # 누적할 값 컬럼들\n    var_name='LOGIN_STATUS',            # 로그인 상태를 나타낼 새 컬럼 이름\n    value_name='COUNT'                  # 로그인 횟수 값을 저장할 새 컬럼 이름\n)\n\n# (선택 사항) 로그인 상태 순서 지정 (예: Successful -> Failed)\nstatus_order = ['SUCCESS', 'FAILED']\nlogins_df_melted['LOGIN_STATUS'] = pd.Categorical(\n    logins_df_melted['LOGIN_STATUS'], categories=status_order, ordered=True\n)\nlogins_df_melted = logins_df_melted.sort_values(by=['USER_NAME', 'LOGIN_STATUS'])\n\n\n# --- 누적 막대 차트 생성 ---\nfig_logins_stacked = px.bar(\n    logins_df_melted,\n    x='USER_NAME',        # x축: 사용자 이름\n    y='COUNT',            # y축: 로그인 횟수\n    color='LOGIN_STATUS', # 이 컬럼 기준으로 막대를 누적하고 색상 구분\n    title=\"Sucessful & Failed login by Users\", # 차트 제목\n    orientation='v',      # 세로 막대 차트\n    hover_data={          # 마우스 호버 시 표시될 정보 설정\n        'USER_NAME': True,\n        'LOGIN_STATUS': True,\n        'COUNT': ':,d'    # 로그인 횟수 (정수형, 콤마 구분 포맷)\n    },\n    labels={              # 축 및 범례/호버 레이블 이름 설정\n        'USER_NAME': '사용자 이름',\n        'COUNT': '로그인 횟수',\n        'LOGIN_STATUS': '로그인 상태'\n    },\n    color_discrete_map={ # 색상 지정 (선택 사항)\n        'SUCCESS': 'lightgreen',\n        'FAILED': 'red'\n    }\n    # barmode='stack' # 'color' 사용 시 기본값이 'stack'이므로 명시적으로 지정할 필요 없음\n)\n\n# (선택 사항) x축 순서 정렬 (예: 총 로그인 횟수 기준 내림차순)\n# total_logins = logins_df_wide.set_index('USER_NAME')[['SUCCESSFUL', 'FAILED']].sum(axis=1).sort_values(ascending=False)\n# fig_logins_stacked.update_layout(xaxis={'categoryorder':'array', 'categoryarray': total_logins.index})\n\n\n# --- Streamlit에 차트 표시 ---\nst.plotly_chart(fig_logins_stacked, use_container_width=True) # Streamlit 환경에서 사용\n\n#fig_logins_stacked.show() # 로컬 환경 등에서 확인시",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b01f089-4010-47af-9506-2a6d290c3ae2",
   "metadata": {
    "language": "sql",
    "name": "Login_Client_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Logins by Client \nselect reported_client_type as Client, \n       user_name, \n       sum(iff(is_success = 'NO', 1, 0)) as Failed, \n       count(*) as Success, \n       sum(iff(is_success = 'NO', 1, 0)) / nullif(count(*), 0) as login_failure_rate \n  from snowflake.account_usage.login_history \n where true\n   and event_timestamp between '{{s}}' and '{{e}}' --Added by JH, 2025-04-27 \n group by 1, 2 \n order by 5 desc ;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9497cc35-71a5-4c24-9704-cfd4cd646e0e",
   "metadata": {
    "language": "python",
    "name": "Login_Client_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('지정된 기간동안 클라이언트 유형별, 사용자별 로그인 성공/실패 횟수')\n\n# logins_client_df = Login_Client_SQL.to_pandas()\n\n# fig_logins_client=px.bar(logins_client_df,x='CLIENT',y='SUCCESS', orientation='v',title=\"Logins by Client\")\n# fig_logins_client.update_traces(marker_color='purple')\n\n# Modified by JH, 2025-04-27\nlogins_client_df_wide = Login_Client_SQL.to_pandas()\n\nlogins_client_df_melted = pd.melt(\n    logins_client_df_wide,\n    id_vars=['CLIENT', 'USER_NAME'],        # 기준이 되는 컬럼들\n    value_vars=['SUCCESS', 'FAILED'],    # 그룹화/색상 구분할 값 컬럼들\n    var_name='LOGIN_STATUS',                # 로그인 상태를 나타낼 새 컬럼 이름\n    value_name='COUNT'                      # 로그인 횟수 값을 저장할 새 컬럼 이름\n)\n\n# (선택 사항) 로그인 상태 순서 지정 (예: Successful -> Failed)\nstatus_order = ['SUCCESS', 'FAILED']\nlogins_client_df_melted['LOGIN_STATUS'] = pd.Categorical(\n    logins_client_df_melted['LOGIN_STATUS'], categories=status_order, ordered=True\n)\nlogins_client_df_melted = logins_client_df_melted.sort_values(by=['CLIENT', 'USER_NAME', 'LOGIN_STATUS'])\n\nfig_logins_client_grouped = px.bar(\n    logins_client_df_melted,\n    x='USER_NAME',        # 각 패싯 내 x축: 사용자 이름\n    y='COUNT',            # y축: 로그인 횟수\n    color='LOGIN_STATUS', # 막대 색상 구분: 로그인 상태 (성공/실패)\n    facet_col='CLIENT',   # 패싯 열: 클라이언트 유형별로 하위 그래프 생성\n    facet_col_wrap=3,     # 한 줄에 표시할 최대 패싯 수 (조정 가능)\n    title=\"Logins by Client\", # 차트 제목\n    hover_data={          # 마우스 호버 시 표시될 정보 설정\n        'CLIENT': True,\n        'USER_NAME': True,\n        'LOGIN_STATUS': True,\n        'COUNT': ':,d'    # 로그인 횟수 (정수형, 콤마 구분 포맷)\n    },\n    labels={              # 축 및 범례/호버 레이블 이름 설정\n        'USER_NAME': 'Username',\n        'COUNT': 'Number of logins',\n        'LOGIN_STATUS': 'Login status',\n        'CLIENT': 'Client type'\n    },\n    color_discrete_map={ # 색상 지정 (선택 사항)\n        'SUCCESS': 'mediumseagreen', # 성공 색상 변경\n        'FAILED': 'tomato'            # 실패 색상 변경\n    },\n    height=500 # 차트 높이 조절 (패싯 수에 따라 조정)\n)\n\n# 패싯 제목 스타일 조정 (선택 사항)\nfig_logins_client_grouped.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n# x축 레이블 각도 조정 (사용자 이름이 많을 경우)\nfig_logins_client_grouped.update_xaxes(tickangle=45)\n\nst.plotly_chart(fig_logins_client_grouped, use_container_width=True) # Streamlit 환경에서 사용\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "547fa907-0e2f-48ee-8191-88246e1ae6ab",
   "metadata": {
    "language": "sql",
    "name": "Never_Login_Since_Created_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Never Logged In Users\nSHOW USERS;\nSELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\nWHERE \"last_success_login\" IS NULL\nAND DATEDIFF('Day',\"created_on\",CURRENT_DATE) > 30;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa9f461f-6013-4812-b334-51eed50f4cae",
   "metadata": {
    "language": "python",
    "name": "Never_Login_Since_Created_Python"
   },
   "outputs": [],
   "source": "st.title('생성 후 30일 이내 로그인한 이력이 없는 사용자 ')\n\ndf_users = Never_Login_Since_Created_SQL.to_pandas()\n\nif not df_users.empty:\n    df_users['created_on'] = pd.to_datetime(df_users['created_on'])\n\n    st.subheader(\"사용자 생성 타임라인\")\n\n    # --- Plotly Express로 산점도 생성 ---\n    fig_timeline = px.scatter(\n        df_users, \n        x='created_on',     \n        y='name',           \n        title='Inactive User Creation Timeline',\n        labels={            \n            'created_on': 'Creation Date/Time',\n            'name': 'User Name'\n        },\n        hover_name='name',  \n        hover_data={       \n            'created_on': '|%Y-%m-%d %H:%M:%S', \n            'name': False \n        }\n    )\n\n    # --- 차트 레이아웃 커스터마이징 ---\n    fig_timeline.update_layout(\n        xaxis_title='Creation Date/Time',\n        yaxis_title='User Name',\n        height=max(400, len(df_users['name'].unique()) * 20) # 사용자 수에 따라 높이 조절\n        # Y축 사용자 이름을 알파벳 오름차순으로 정렬\n        # yaxis_categoryorder='category ascending' # 또는 category descending\n    )\n    # Y축 레이블 정렬 (알파벳 순) - 필요시 주석 해제\n    fig_timeline.update_yaxes(categoryorder='category ascending')\n\n    # 점(마커) 스타일 변경 (선택 사항)\n    fig_timeline.update_traces(marker=dict(size=8, symbol='circle'))\n\n    # --- Streamlit에 차트 표시 ---\n    st.plotly_chart(fig_timeline, use_container_width=True)\n\nelse:\n    st.warning(\"조건에 맞는 사용자가 없습니다.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4014ca0f-6a00-419f-9843-2ab833c4d3c9",
   "metadata": {
    "language": "sql",
    "name": "Infrequent_User_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--Stale Users\nSHOW USERS;\nSELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\nWHERE DATEDIFF('Day',\"last_success_login\",CURRENT_DATE) > 30;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e97c5fab-fa77-4573-ba0a-dfacf37a8942",
   "metadata": {
    "language": "python",
    "name": "Infrequent_User_Python"
   },
   "outputs": [],
   "source": "st.title('지난 30일 이내 새로 로그인하지 않은 사용자')\n\ndf_users = Infrequent_User_SQL.to_pandas()\n\nif not df_users.empty:\n    df_users['last_success_login'] = pd.to_datetime(df_users['last_success_login'])\n\n    st.subheader(\"사용자 생성 타임라인\")\n\n    # --- Plotly Express로 산점도 생성 ---\n    fig_timeline = px.scatter(\n        df_users, \n        x='last_success_login',     \n        y='name',           \n        title='Infrequent User',\n        labels={            \n            'last_success_login': 'last success login Date/Time',\n            'name': 'User Name'\n        },\n        hover_name='name',  \n        hover_data={       \n            'last_success_login': '|%Y-%m-%d %H:%M:%S', \n            'name': False \n        }\n    )\n\n    # --- 차트 레이아웃 커스터마이징 ---\n    fig_timeline.update_layout(\n        xaxis_title='last success login Date/Time',\n        yaxis_title='User Name',\n        height=max(400, len(df_users['name'].unique()) * 20) # 사용자 수에 따라 높이 조절\n        # Y축 사용자 이름을 알파벳 오름차순으로 정렬\n        # yaxis_categoryorder='category ascending' # 또는 category descending\n    )\n    # Y축 레이블 정렬 (알파벳 순) - 필요시 주석 해제\n    fig_timeline.update_yaxes(categoryorder='category ascending')\n\n    # 점(마커) 스타일 변경 (선택 사항)\n    fig_timeline.update_traces(marker=dict(size=8, symbol='circle'))\n\n    # --- Streamlit에 차트 표시 ---\n    st.plotly_chart(fig_timeline, use_container_width=True)\n\nelse:\n    st.warning(\"조건에 맞는 사용자가 없습니다.\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "752838d5-845c-448d-b1be-6edba30f2921",
   "metadata": {
    "name": "Tables",
    "collapsed": false
   },
   "source": "# 테이블 분석"
  },
  {
   "cell_type": "code",
   "id": "ecdb36b8-c873-4d91-9838-ae64c66612e8",
   "metadata": {
    "language": "sql",
    "name": "HighChurn_ShortLived_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n-- Identify high churn tables or short lived tables\nSELECT\n        t.table_catalog||'.'||t.table_schema||'.'||t.table_name as fq_table_name\n       ,t.table_catalog as database\n       ,t.table_schema as schema\n       ,t.table_name as table_name\n       ,t.active_bytes/power(1024,3) as active_size_gb\n       ,t.time_travel_bytes/power(1024,3) as time_travel_gb\n       ,t.failsafe_bytes/power(1024,3) as failsafe_gb\n       ,t.retained_for_clone_bytes/power(1024,3) as clone_retain_gb\n       ,active_size_gb+time_travel_gb+failsafe_gb+clone_retain_gb as total_size_gb\n       ,(t.time_travel_bytes + t.failsafe_bytes + t.retained_for_clone_bytes)/power(1024,3) as non_active_size_gb\n       ,div0(non_active_size_gb,active_size_gb)*100 as churn_pct\n       ,t.deleted\n       ,timediff('hour',t.table_created,t.table_dropped) as table_life_duration_hours\n       ,t1.is_transient\n       ,t1.table_type\n       ,t1.retention_time\n       ,t1.auto_clustering_on\n       ,t1.clustering_key\n       ,t1.last_altered\n       ,t1.last_ddl\n   FROM snowflake.account_usage.table_storage_metrics t\n        JOIN snowflake.account_usage.tables t1\n          ON t.id=t1.table_id\n  WHERE 1=1\n        AND t1.table_catalog not in ('SNOWFLAKE') -- use this to filter on specific databases\n        AND \n            (\n             churn_pct>=40\n             OR\n             table_life_duration_hours<=24  -- short lived tables\n            )\n  ORDER BY total_size_gb desc ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "faa46d7e-f599-491e-8eb4-a1c66ce24b16",
   "metadata": {
    "language": "python",
    "name": "HighChurn_ShortLived_Python"
   },
   "outputs": [],
   "source": "# Modifed by JH, 2025-05-2\n\n# Streamlit 앱 제목 설정\nst.title('짧은 생명 주기의 테이블 리스트')\n\n# 가정: HighChurn_ShortLived_SQL 변수에 쿼리 결과가 있고, 이를 Pandas DataFrame으로 변환합니다.\nHighChurn_ShortLived_df = HighChurn_ShortLived_SQL.to_pandas()\n\n# 1. 데이터베이스별 테이블 수 시각화 (막대 그래프)\nst.subheader(\"데이터베이스별 테이블 수\")\nfig_db = px.bar(HighChurn_ShortLived_df, x='DATABASE', color='DATABASE',\n             title='데이터베이스별 테이블 수',\n             labels={'DATABASE': '데이터베이스', 'count': '테이블 수'})\nfig_db.update_layout(xaxis_tickangle=-45)\nst.plotly_chart(fig_db)\n\n# 2. 스키마별 테이블 수 시각화 (막대 그래프)\nst.subheader(\"스키마별 테이블 수\")\nfig_schema = px.bar(HighChurn_ShortLived_df, x='SCHEMA', color='SCHEMA',\n                 title='스키마별 테이블 수',\n                 labels={'SCHEMA': '스키마', 'count': '테이블 수'})\nfig_schema.update_layout(xaxis_tickangle=-45)\nst.plotly_chart(fig_schema)\n\n# 3. 테이블별 총 크기 시각화 (수평 막대 그래프 - 테이블 이름 확인 용이)\nst.subheader(\"테이블별 총 크기 (GB)\")\ndf_sorted = HighChurn_ShortLived_df.sort_values(by='TOTAL_SIZE_GB', ascending=False)\nfig_size = px.bar(df_sorted, y='DATABASE', x='TOTAL_SIZE_GB', color='TABLE_NAME',\n                 title='테이블별 총 크기 (GB)',\n                 labels={'DATABASE': '데이터베이스', 'TOTAL_SIZE_GB': '총 크기 (GB)', 'TABLE_NAME': '테이블 이름'},\n                 orientation='h')\nfig_size.update_layout(yaxis={'categoryorder':'array', 'categoryarray': df_sorted['DATABASE'].unique()})\nst.plotly_chart(fig_size)\n\n# 추가적으로 테이블 이름과 크기를 함께 보여주는 테이블 형태 (plotly.graph_objects 사용)\nst.subheader(\"테이블 이름 및 총 크기\")\nfig_table = go.Figure(data=[go.Table(\n    header=dict(values=list(HighChurn_ShortLived_df[['DATABASE', 'SCHEMA', 'TABLE_NAME', 'DELETED','IS_TRANSIENT','LAST_ALTERED','LAST_DDL']].columns),\n                align='left'),\n    cells=dict(values=[HighChurn_ShortLived_df['DATABASE'], HighChurn_ShortLived_df['SCHEMA'], HighChurn_ShortLived_df['TABLE_NAME'], HighChurn_ShortLived_df['DELETED'],HighChurn_ShortLived_df['IS_TRANSIENT'],HighChurn_ShortLived_df['LAST_ALTERED'], HighChurn_ShortLived_df['LAST_DDL']],\n               align='left'))\n])\nfig_table.update_layout(title='테이블 이름 및 총 크기')\nst.plotly_chart(fig_table)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f220a636-c26e-40fc-8bdc-87b34cea79c6",
   "metadata": {
    "language": "sql",
    "name": "Unused_Table_1W_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Unused tables\n-- Identify Table sizes and Last DDL/DML Timestamps\nSELECT TABLE_CATALOG || '.' || TABLE_SCHEMA || '.' || TABLE_NAME AS TABLE_PATH\n      ,TABLE_CATALOG AS DATABASE\n      ,TABLE_SCHEMA AS SCHEMA\n      ,TABLE_NAME\n      ,BYTES\n      ,TO_NUMBER(BYTES / POWER(1024,3),10,2) AS GB\n      ,LAST_ALTERED AS LAST_USE\n      ,DATEDIFF('Day',LAST_USE,CURRENT_DATE) AS DAYS_SINCE_LAST_USE\n  FROM INFORMATION_SCHEMA.TABLES\n WHERE DAYS_SINCE_LAST_USE > 7 --Use your Days Threshold\n ORDER BY DATABASE, SCHEMA, TABLE_NAME, DAYS_SINCE_LAST_USE DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2177f081-6752-49d8-8e53-f50964da8863",
   "metadata": {
    "language": "python",
    "name": "Unused_Table_1W_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.title('7일 이상 사용되지 않은 테이블 리스트')\n\nUnusedTable_df = Unused_Table_1W_SQL.to_pandas()\n\n# 선버스트 차트에서 각 테이블의 크기를 동일하게 표현하기 위해 카운트 컬럼 추가\n# UnusedTable_df['COUNT'] = 1\n\n# # --- 선버스트 차트(Sunburst Chart) 생성 ---\n# fig_db_schema_table = px.sunburst(\n#     UnusedTable_df,\n#     path=['DATABASE', 'SCHEMA', 'TABLE_NAME'], # 계층 구조 정의: 데이터베이스 -> 스키마 -> 테이블\n#     values='COUNT',                            # 각 조각(테이블)의 크기 (여기서는 동일하게 1로 설정)\n#     title='Unused Table List',                 # 차트 제목\n#     # color='DATABASE_NAME',                   # 최상위 레벨(데이터베이스) 기준으로 색상 구분 (선택 사항)\n#     # maxdepth=2                               # 표시할 최대 깊이 설정 (예: 스키마까지만 보려면 2) (선택 사항)\n# )\n\n# # 차트 레이아웃 업데이트 (선택 사항)\n# fig_db_schema_table.update_layout(\n#     margin=dict(t=50, l=25, r=25, b=25) # 여백 조정\n# )\n\n# # 호버 정보 업데이트 (선택 사항)\n# fig_db_schema_table.update_traces(\n#     hovertemplate='<b>%{label}</b><br>경로: %{id}<extra></extra>' # 호버 시 표시될 텍스트 형식\n# )\n\n# # --- Streamlit에 차트 표시 ---\n# st.plotly_chart(fig_db_schema_table, use_container_width=True) # Streamlit 환경에서 사용\n\ndf_sorted = UnusedTable_df.sort_values(by='GB', ascending=False)\nfig_size = px.bar(df_sorted, y='DATABASE', x='GB', color='TABLE_PATH',\n                 title='테이블별 총 크기 (GB)',\n                 labels={'DATABASE': '데이터베이스', 'GB': '총 크기 (GB)', 'TABLE_PATH': '테이블 이름'},\n                 orientation='h')\nfig_size.update_layout(yaxis={'categoryorder':'array', 'categoryarray': df_sorted['DATABASE'].unique()})\nst.plotly_chart(fig_size)\n\n# 추가적으로 테이블 이름과 크기를 함께 보여주는 테이블 형태 (plotly.graph_objects 사용)\nfig_table = go.Figure(data=[go.Table(\n    header=dict(values=list(UnusedTable_df[['DATABASE', 'SCHEMA', 'TABLE_NAME','GB']].columns),\n                align='left'),\n    cells=dict(values=[UnusedTable_df['DATABASE'], UnusedTable_df['SCHEMA'], UnusedTable_df['TABLE_NAME'], UnusedTable_df['GB']],\n               align='left'))\n])\nfig_table.update_layout(title='테이블 이름 및 총 크기')\nst.plotly_chart(fig_table)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "562fccb0-07cd-4c9e-8782-f6b2e12a67e4",
   "metadata": {
    "language": "sql",
    "name": "Tables_Used_In_Query_SQL",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Tables used in any query in the last 7 days\nWITH access_history as\n(   \nSELECT  \n       distinct split(base.value:objectName, '.')[0]::string as DATABASE\n       ,split(base.value:objectName, '.')[1]::string as SCHEMA\n       ,split(base.value:objectName, '.')[2]::string as TABLE_NAME\n  FROM snowflake.account_usage.access_history \n       ,lateral flatten (base_objects_accessed) base\n where query_start_time between current_date()-7 and current_date()\n)\nSELECT tbl.table_catalog||'.'||tbl.table_schema||'.'||tbl.table_name as FQ_table_name,\n       ah.database,\n       ah.schema,\n       ah.table_name,\n       TO_NUMBER(tbl.bytes / POWER(1024,3),10,2) AS GB\n  FROM snowflake.account_usage.tables tbl\n  LEFT JOIN access_history ah\n    ON tbl.table_name=ah.table_name\n   AND tbl.table_schema=ah.schema\n   AND tbl.table_catalog=ah.database\n WHERE ah.table_name is not NULL\n   AND tbl.deleted is null \n;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81616218-c7c5-4d0f-9a0c-9cd5fbde6838",
   "metadata": {
    "language": "python",
    "name": "Tables_Used_In_Query_Python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# st.title('지난 7일간 쿼리에서 더 이상 사용되지 않은 테이블 리스트')\n\n# TablesNotUsedInQuery_df = Tables_Not_Used_In_Query_SQL.to_pandas()\n\n# # 선버스트 차트에서 각 테이블의 크기를 동일하게 표현하기 위해 카운트 컬럼 추가\n# TablesNotUsedInQuery_df['COUNT'] = 1\n\n# # --- 선버스트 차트(Sunburst Chart) 생성 ---\n# fig_db_schema_table = px.sunburst(\n#     TablesNotUsedInQuery_df,\n#     path=['DATABASE', 'SCHEMA', 'TABLE_NAME'],               # 계층 구조 정의: 데이터베이스 -> 스키마 -> 테이블\n#     values='COUNT',                                          # 각 조각(테이블)의 크기 (여기서는 동일하게 1로 설정)\n#     title='Tables not used in any query in the last 7 days', # 차트 제목\n#     # color='DATABASE_NAME',                                 # 최상위 레벨(데이터베이스) 기준으로 색상 구분 (선택 사항)\n#     # maxdepth=2                                             # 표시할 최대 깊이 설정 (예: 스키마까지만 보려면 2) (선택 사항)\n# )\n\n# # 차트 레이아웃 업데이트 (선택 사항)\n# fig_db_schema_table.update_layout(\n#     margin=dict(t=50, l=25, r=25, b=25) # 여백 조정\n# )\n\n# # 호버 정보 업데이트 (선택 사항)\n# fig_db_schema_table.update_traces(\n#     hovertemplate='<b>%{label}</b><br>경로: %{id}<extra></extra>' # 호버 시 표시될 텍스트 형식\n# )\n\n\n# # --- Streamlit에 차트 표시 ---\n# st.plotly_chart(fig_db_schema_table, use_container_width=True) # Streamlit 환경에서 사용\n\n# modified by JH, 2025-05-02\nst.title('지난 7일간 쿼리에서 사용된 테이블 리스트')\n\nTablesUsedInQuery_df = Tables_Used_In_Query_SQL.to_pandas()\n\ndf_sorted = TablesUsedInQuery_df.sort_values(by='GB', ascending=False)\nfig_size = px.bar(df_sorted, y='DATABASE', x='GB', color='TABLE_NAME',\n                 title='테이블별 총 크기 (GB)',\n                 labels={'DATABASE': '데이터베이스', 'GB': '총 크기 (GB)', 'TABLE_NAME': '테이블 이름'},\n                 orientation='h')\nfig_size.update_layout(yaxis={'categoryorder':'array', 'categoryarray': df_sorted['DATABASE'].unique()})\nst.plotly_chart(fig_size)\n\nfig_table = go.Figure(data=[go.Table(\n    header=dict(values=list(df_sorted[['DATABASE', 'SCHEMA', 'TABLE_NAME','GB']].columns),\n                align='left'),\n    cells=dict(values=[df_sorted['DATABASE'], df_sorted['SCHEMA'], df_sorted['TABLE_NAME'], df_sorted['GB']],\n               align='left'))\n])\nfig_table.update_layout(title='테이블 이름 및 총 크기')\nst.plotly_chart(fig_table)",
   "execution_count": null
  }
 ]
}