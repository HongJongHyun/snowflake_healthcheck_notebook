{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {
    "codeCollapsed": true
   },
   "source": "# Snowflake Account Usage - Workspace Version\n\n### ì´ ì•±ì€ ì—¬ëŸ¬ë¶„ì˜ Snowflake ê³„ì •ì— ìˆëŠ” account_usage ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. \n\nì´ ë…¸íŠ¸ë¶ì€ Snowflake Notebooksì—ì„œ VS Code Workspaceìš©ìœ¼ë¡œ ë³€í™˜ëœ ë²„ì „ì…ë‹ˆë‹¤.\nSQL ì…€ ëŒ€ì‹  Pythonì—ì„œ ì§ì ‘ SQLì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n\nìì„¸í•œ ì •ë³´ëŠ” ì•„ë˜ Snowflake ê³µì‹ ë¬¸ì„œ í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.  \nhttps://docs.snowflake.com/en/sql-reference/account-usage#account-usage-views"
  },
  {
   "cell_type": "markdown",
   "id": "package-period",
   "metadata": {
    "codeCollapsed": true
   },
   "source": "# íŒ¨í‚¤ì§€ ì„¤ì • ë° ì¸¡ì • ë‚ ì§œ ì§€ì •"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-package",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "%pip install pandas plotly altair"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packages",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport plotly.express as px \nimport datetime\nimport altair as alt\nimport plotly.graph_objects as go \nimport numpy as np\nfrom IPython.display import display, Markdown\n\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\ndef run_sql(sql):\n    \"\"\"Execute SQL and return pandas DataFrame\"\"\"\n    return session.sql(sql).to_pandas()\n\ndef title(text):\n    \"\"\"Display title\"\"\"\n    display(Markdown(f\"## {text}\"))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "date-filter",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ëª¨ë‹ˆí„°ë§ ë‚ ì§œ ë²”ìœ„ ì§€ì •')\n\nDAYS = 30\n\ntoday = datetime.date.today()\ns = (today - datetime.timedelta(days=DAYS)).strftime('%Y-%m-%d')\ne = today.strftime('%Y-%m-%d')\n\nprint(f\"ë¶„ì„ ê¸°ê°„: {s} ~ {e}\")\nprint(f\"\\nâ€» ê¸°ê°„ ë³€ê²½í•˜ë ¤ë©´ ìœ„ì˜ DAYS ê°’ì„ ìˆ˜ì •í•˜ì„¸ìš” (7, 14, 30, 60, 120)\")"
  },
  {
   "cell_type": "markdown",
   "id": "overviews",
   "metadata": {
    "codeCollapsed": true
   },
   "source": "# ì‚¬ìš©ëŸ‰ ê°œìš”"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cards",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('Account ì‚¬ìš©ëŸ‰ ê°œìš”')\n\npandas_credits_used_df = run_sql(f\"\"\"\n    SELECT round(sum(credits_used),0) as total_credits \n    FROM snowflake.account_usage.metering_history \n    WHERE start_time between '{s}' and '{e}'\n\"\"\")\n\npandas_num_jobs_df = run_sql(f\"\"\"\n    SELECT count(*) as number_of_jobs \n    FROM snowflake.account_usage.query_history \n    WHERE start_time between '{s}' and '{e}'\n\"\"\")\n\npandas_current_storage_df = run_sql(\"\"\"\n    SELECT round(avg(storage_bytes + stage_bytes + failsafe_bytes) / power(1024, 4),2) as billable_tb \n    FROM snowflake.account_usage.storage_usage \n    WHERE USAGE_DATE = current_date() - 1\n\"\"\")\n\ncredits_used_tile = pandas_credits_used_df.iloc[0].values[0]\nnum_jobs_tile = pandas_num_jobs_df.iloc[0].values[0]\ncurrent_storage_tile = pandas_current_storage_df.iloc[0].values[0]\n\nprint(f\"ğŸ“Š Credits Used: {int(credits_used_tile):,}\")\nprint(f\"ğŸ“Š Total # of Jobs Executed: {int(num_jobs_tile):,}\")\nprint(f\"ğŸ“Š Current Storage (TB): {current_storage_tile}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "credits-by-month",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì›”ë³„ ì „ì²´ í¬ë ˆë”§ ì‚¬ìš©ëŸ‰ ì¶”ì´')\n\ncredits_billed_df = run_sql(\"\"\"\n    SELECT date_trunc('MONTH', usage_date) as Usage_Month, \n           sum(CREDITS_BILLED) as sum_credits\n    FROM snowflake.account_usage.metering_daily_history \n    WHERE usage_date >= DATEADD('MONTH', -12, DATE_TRUNC('MONTH', CURRENT_TIMESTAMP()))\n    GROUP BY Usage_Month\n\"\"\")\n\nfig_credits_billed=px.bar(credits_billed_df,x='USAGE_MONTH',y='SUM_CREDITS', orientation='v',title=\"Credits Billed by Month\")\nfig_credits_billed.show()\n\nprint('The above chart is static and not modified by the date range filter')"
  },
  {
   "cell_type": "markdown",
   "id": "virtual-warehouse",
   "metadata": {
    "codeCollapsed": true
   },
   "source": "# ê°€ìƒ ì›¨ì–´í•˜ìš°ìŠ¤ (Virtual Warehouse) ë¶„ì„"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jobs-wh",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ì •ëœ ê¸°ê°„ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ ìˆ˜í–‰ëœ ì‘ì—…ì˜ ìˆ˜')\n\npandas_jobs_by_warehouse_df = run_sql(f\"\"\"\n    SELECT warehouse_name, count(*) as number_of_jobs \n    FROM snowflake.account_usage.query_history \n    WHERE start_time between '{s}' and '{e}' \n      AND warehouse_name is not null\n    GROUP BY 1 \n    ORDER BY 2 desc \n    LIMIT 10\n\"\"\")\n\npandas_jobs_by_warehouse_df_sorted = pandas_jobs_by_warehouse_df.sort_values(by='NUMBER_OF_JOBS', ascending=True)\n\nfig_jobs_by_warehouse=px.bar(pandas_jobs_by_warehouse_df_sorted,x='NUMBER_OF_JOBS',y='WAREHOUSE_NAME',orientation='h',title=\"# of Jobs by Warehouse\")\nfig_jobs_by_warehouse.update_traces(marker_color='purple')\nfig_jobs_by_warehouse.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "credits-used-overtime",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ì •ëœ ê¸°ê°„ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ í¬ë ˆë”§ ì‚¬ìš©ì–‘ ì¶”ì„¸')\n\npandas_credits_used_overtime_df = run_sql(f\"\"\"\n    SELECT start_time::date as usage_date, warehouse_name, sum(credits_used) as total_credits_used \n    FROM snowflake.account_usage.warehouse_metering_history \n    WHERE start_time between '{s}' and '{e}' \n    GROUP BY 1,2 \n    HAVING total_credits_used > 0.001\n    ORDER BY 2,1\n\"\"\")\n\nfig_credits_used_overtime_df=px.bar(pandas_credits_used_overtime_df,x='USAGE_DATE',y='TOTAL_CREDITS_USED',color='WAREHOUSE_NAME',orientation='v',title=\"Credits Used Overtime\")\nfig_credits_used_overtime_df.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a5169",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ì •ëœ ê¸°ê°„ ë™ì•ˆ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ ì›Œí¬ë¡œë“œ ìˆ˜í–‰ì‹œê°„ ë¶„í¬')\n\nExeTimeInWH_df = run_sql(f\"\"\"\n    WITH base_table as (\n        SELECT warehouse_name, \n               total_elapsed_time,\n               CASE\n                    WHEN total_elapsed_time < 1 THEN 'a'\n                    WHEN total_elapsed_time < 10 THEN 'b'\n                    WHEN total_elapsed_time < 60 THEN 'c'\n                    WHEN total_elapsed_time < 300 THEN 'd'\n                    WHEN total_elapsed_time < 600 THEN 'e'\n                    WHEN total_elapsed_time > 600 THEN 'f'\n               END as exe_time\n        FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\n        WHERE warehouse_name is not null\n          AND start_time between '{s}' and '{e}'\n          AND warehouse_name not ilike 'compute_service%'   \n    )\n    SELECT warehouse_name, \n           sum(iff(exe_time= 'a', 1, 0)) U00_01S,\n           sum(iff(exe_time= 'b', 1, 0)) U01_10S,\n           sum(iff(exe_time= 'c', 1, 0)) U10_60S,\n           sum(iff(exe_time= 'd', 1, 0)) U01_05M,\n           sum(iff(exe_time= 'e', 1, 0)) U05_10M,\n           sum(iff(exe_time= 'f', 1, 0)) U10M_\n    FROM base_table\n    GROUP BY warehouse_name\n\"\"\")\n\nplot_columns = ['U00_01S', 'U01_10S', 'U10_60S', 'U01_05M', 'U05_10M', 'U10M_']\n\nlabels_map = {\n    'WAREHOUSE_NAME': 'Warehouse Name',\n    'variable': 'Execution Time Bucket',\n    'value': 'Number of Queries',\n    'U00_01S': '< 1s',\n    'U01_10S': '1s - 10s',\n    'U10_60S': '10s - 60s',\n    'U01_05M': '1m - 5m',\n    'U05_10M': '5m - 10m',\n    'U10M_': '> 10m'\n}\n\ndf_long = pd.melt(ExeTimeInWH_df,\n                  id_vars='WAREHOUSE_NAME',\n                  value_vars=plot_columns,\n                  var_name='variable',\n                  value_name='value')\n\nfig = px.bar(\n    df_long,\n    x='WAREHOUSE_NAME',\n    y='value',\n    color='variable',\n    title='Query Execution Time Distribution by Warehouse',\n    labels=labels_map,\n    height=500,\n    category_orders={'variable': plot_columns}\n)\n\nfig.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6f04c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì¼ë³„ ì›¨ì–´í•˜ìš°ìŠ¤ì˜ ì›Œí¬ë¡œë“œ ì¶”ì´ (ì§€ë‚œ 1ê°œì›”)')\n\nRunQueueWH_df = run_sql(\"\"\"\n    SELECT to_date(start_time) as date,\n           warehouse_name,\n           sum(avg_running) as sum_running,\n           sum(avg_queued_load) as sum_queued\n    FROM snowflake.account_usage.warehouse_load_history\n    WHERE to_date(start_time) >= dateadd(month, -1, current_timestamp())\n    GROUP BY 1,2\n    ORDER BY 1,2\n\"\"\")\n\nfig_running = px.line(\n    RunQueueWH_df, x='DATE', y='SUM_RUNNING', color='WAREHOUSE_NAME',\n    title='Daily Sum of Average Running Queries by Warehouse',\n    labels={'DATE': 'Date', 'SUM_RUNNING': 'Sum of Avg Running Queries', 'WAREHOUSE_NAME': 'Warehouse'},\n    markers=True\n)\nfig_running.update_layout(hovermode='x unified')\nfig_running.show()\n\nfig_queued = px.line(\n    RunQueueWH_df, x='DATE', y='SUM_QUEUED', color='WAREHOUSE_NAME',\n    title='Daily Sum of Average Queued Queries by Warehouse',\n    labels={'DATE': 'Date', 'SUM_QUEUED': 'Sum of Avg Queued Queries', 'WAREHOUSE_NAME': 'Warehouse'},\n    markers=True\n)\nfig_queued.update_layout(hovermode='x unified')\nfig_queued.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01507cdb",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ê°€ì¥ ì˜¤ë˜ ìˆ˜í–‰ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸')\n\npandas_longest_queries_df = run_sql(f\"\"\"\n    SELECT query_id, query_text, (execution_time / 1000) as exec_time \n    FROM snowflake.account_usage.query_history \n    WHERE execution_status = 'SUCCESS' \n      AND start_time between '{s}' and '{e}' \n      AND query_text != ''\n      AND exec_time > 10\n    ORDER BY execution_time desc \n    LIMIT 25\n\"\"\")\n\nmax_len = 100\ncol_orig = 'QUERY_TEXT'\ncol_short = 'QUERY_TEXT_SHORT'\n\npandas_longest_queries_df[col_short] = pandas_longest_queries_df[col_orig].str.slice(0, max_len)\npandas_longest_queries_df.loc[pandas_longest_queries_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_longest_queries = px.bar(\n    pandas_longest_queries_df, x='EXEC_TIME', y=col_short, orientation='h',\n    title=\"Longest Successful Queries (Top 25)\",\n    hover_data={'EXEC_TIME': ':.2f s', col_orig: True, col_short: False},\n    labels={col_short: \"Query Text (Truncated)\"}\n)\n\nfig_longest_queries.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a796bcf",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ì •ëœ ê¸°ê°„ë‚´ì˜ ë°˜ë³µë˜ëŠ” ì¿¼ë¦¬ ìˆ˜í–‰')\n\nTotal_Execution_Time_df = run_sql(f\"\"\"\n    SELECT query_text, \n           (sum(execution_time) / 1000) as exec_time,\n           count(*) as number_of_query\n    FROM snowflake.account_usage.query_history \n    WHERE execution_status = 'SUCCESS' \n      AND start_time between '{s}' and '{e}' \n      AND query_text != ''\n    GROUP BY query_text \n    ORDER BY exec_time desc \n    LIMIT 10\n\"\"\")\n\nmax_len = 100\ncol_orig = 'QUERY_TEXT'\ncol_short = 'QUERY_TEXT_SHORT'\n\nTotal_Execution_Time_df[col_short] = Total_Execution_Time_df[col_orig].str.slice(0, max_len)\nTotal_Execution_Time_df.loc[Total_Execution_Time_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_Total_Execution_Time = px.bar(\n    Total_Execution_Time_df, x='EXEC_TIME', y=col_short, orientation='h',\n    title=\"Total Execution Time by Repeated Queries\",\n    hover_data={'EXEC_TIME': ':.3f s', col_orig: True, col_short: False, 'NUMBER_OF_QUERY': ':,d'},\n    labels={col_short: \"Query Text (Truncated)\", 'EXEC_TIME': \"Total Execution Time (s)\", 'NUMBER_OF_QUERY': \"Number of Executions\"}\n)\nfig_Total_Execution_Time.update_traces(marker_color='LightSkyBlue')\n\nfig_Total_Execution_Time.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69758f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ì‚¬ìš©ìë³„ í‰ê·  ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')\n\nquery_execution_df = run_sql(f\"\"\"\n    SELECT user_name, (avg(execution_time)) / 1000 as average_execution_time \n    FROM snowflake.account_usage.query_history \n    WHERE execution_status = 'SUCCESS'\n      AND start_time between '{s}' and '{e}'\n    GROUP BY 1 \n    ORDER BY 2 desc \n    LIMIT 10\n\"\"\")\n\nfig_cquery_execution=px.bar(query_execution_df,x='USER_NAME',y='AVERAGE_EXECUTION_TIME', orientation='v',title=\"Average Execution Time per User\")\nfig_cquery_execution.update_traces(marker_color='MediumPurple')\nfig_cquery_execution.show()"
  },
  {
   "cell_type": "markdown",
   "id": "d92555e8",
   "metadata": {
    "codeCollapsed": true
   },
   "source": "# í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì˜ì—­"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06541143",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ í´ë¼ìš°ë“œì„œë¹„ìŠ¤ì˜ ì´ ì‚¬ìš© í¬ë ˆë”§ì–‘')\n\ncompute_gs_by_warehouse_df = run_sql(f\"\"\"\n    SELECT warehouse_name, \n           sum(credits_used_cloud_services) CREDITS_USED_CLOUD_SERVICES \n    FROM snowflake.account_usage.warehouse_metering_history \n    WHERE start_time between '{s}' and '{e}'\n    GROUP BY 1 \n    ORDER BY 2 desc \n    LIMIT 10\n\"\"\")\n\nfig_compute_gs_by_warehouse=px.bar(compute_gs_by_warehouse_df,x='WAREHOUSE_NAME',y='CREDITS_USED_CLOUD_SERVICES', orientation='v',title=\"Compute and Cloud Services by Warehouse\", barmode=\"group\")\nfig_compute_gs_by_warehouse.update_traces(marker_color='purple')\nfig_compute_gs_by_warehouse.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b6a3a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì›”ë³„ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ì¶”ì´')\n\nstorage_overtime_df = run_sql(\"\"\"\n    SELECT date_trunc(month, usage_date) as usage_month, \n           avg(storage_bytes + stage_bytes + failsafe_bytes) / power(1024, 4) as billable_tb, \n           avg(storage_bytes) / power(1024, 4) as Storage_TB, \n           avg(stage_bytes) / power(1024, 4) as Stage_TB, \n           avg(failsafe_bytes) / power(1024, 4) as Failsafe_TB \n    FROM snowflake.account_usage.storage_usage \n    GROUP BY 1 \n    ORDER BY 1\n\"\"\")\n\nstorage_df_melted = pd.melt(storage_overtime_df, id_vars=['USAGE_MONTH'],\n    value_vars=['STORAGE_TB', 'STAGE_TB', 'FAILSAFE_TB'], var_name='STORAGE_TYPE', value_name='SIZE_TB')\n\nfig_stacked_storage = px.bar(storage_df_melted, x='USAGE_MONTH', y='SIZE_TB', color='STORAGE_TYPE',\n    title=\"Data Storage used Overtime\", orientation='v',\n    labels={'USAGE_MONTH': 'Month', 'SIZE_TB': 'Storage Size (TB)', 'STORAGE_TYPE': 'Storage Type'})\n\nfig_stacked_storage.show()\nprint('The above chart is static and not modified by the date range filter')"
  },
  {
   "cell_type": "markdown",
   "id": "dba37d3d",
   "metadata": {
    "codeCollapsed": true
   },
   "source": "# ì‚¬ìš©ì"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb011cf",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ í´ë¼ì´ì–¸íŠ¸ ìœ í˜•ë³„, ì‚¬ìš©ìë³„ ë¡œê·¸ì¸ ì„±ê³µ/ì‹¤íŒ¨ íšŸìˆ˜')\n\nlogins_client_df_wide = run_sql(f\"\"\"\n    SELECT reported_client_type as Client, \n           user_name, \n           sum(iff(is_success = 'NO', 1, 0)) as Failed, \n           count(*) as Success, \n           sum(iff(is_success = 'NO', 1, 0)) / nullif(count(*), 0) as login_failure_rate \n    FROM snowflake.account_usage.login_history \n    WHERE event_timestamp between '{s}' and '{e}'\n    GROUP BY 1, 2 \n    ORDER BY 5 desc\n\"\"\")\n\nlogins_client_df_melted = pd.melt(logins_client_df_wide, id_vars=['CLIENT', 'USER_NAME'],\n    value_vars=['SUCCESS', 'FAILED'], var_name='LOGIN_STATUS', value_name='COUNT')\n\nfig_logins_client_grouped = px.bar(\n    logins_client_df_melted, x='USER_NAME', y='COUNT', color='LOGIN_STATUS',\n    facet_col='CLIENT', facet_col_wrap=3, title=\"Logins by Client\",\n    labels={'USER_NAME': 'Username', 'COUNT': 'Number of logins', 'LOGIN_STATUS': 'Login status', 'CLIENT': 'Client type'},\n    color_discrete_map={'SUCCESS': 'mediumseagreen', 'FAILED': 'tomato'}, height=500)\n\nfig_logins_client_grouped.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119cf0",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ë‚œ 30ì¼ ì´ë‚´ ìƒˆë¡œ ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ì‚¬ìš©ì')\n\nsession.sql(\"SHOW USERS\").collect()\ndf_users_stale = run_sql(\"\"\"\n    SELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\n    WHERE DATEDIFF('Day',\"last_success_login\",CURRENT_DATE) > 30\n\"\"\")\n\nif not df_users_stale.empty:\n    df_users_stale['last_success_login'] = pd.to_datetime(df_users_stale['last_success_login'])\n    print(\"ì‚¬ìš©ì ë§ˆì§€ë§‰ ë¡œê·¸ì¸ íƒ€ì„ë¼ì¸\")\n    \n    fig_timeline = px.scatter(df_users_stale, x='last_success_login', y='name',\n        title='Infrequent User',\n        labels={'last_success_login': 'Last Success Login Date/Time', 'name': 'User Name'}, hover_name='name')\n    fig_timeline.update_layout(height=max(400, len(df_users_stale['name'].unique()) * 20))\n    fig_timeline.update_traces(marker=dict(size=8, symbol='circle'))\n    fig_timeline.show()\nelse:\n    print(\"ì¡°ê±´ì— ë§ëŠ” ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b774d1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§§ì€ ìƒëª… ì£¼ê¸°ì˜ í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸')\n\nHighChurn_ShortLived_df = run_sql(\"\"\"\n    SELECT\n            t.table_catalog||'.'||t.table_schema||'.'||t.table_name as fq_table_name,\n            t.table_catalog as database,\n            t.table_schema as schema,\n            t.table_name as table_name,\n            t.active_bytes/power(1024,3) as active_size_gb,\n            t.time_travel_bytes/power(1024,3) as time_travel_gb,\n            t.failsafe_bytes/power(1024,3) as failsafe_gb,\n            t.retained_for_clone_bytes/power(1024,3) as clone_retain_gb,\n            active_size_gb+time_travel_gb+failsafe_gb+clone_retain_gb as total_size_gb,\n            (t.time_travel_bytes + t.failsafe_bytes + t.retained_for_clone_bytes)/power(1024,3) as non_active_size_gb,\n            div0(non_active_size_gb,active_size_gb)*100 as churn_pct,\n            t.deleted,\n            timediff('hour',t.table_created,t.table_dropped) as table_life_duration_hours,\n            t1.is_transient,\n            t1.table_type,\n            t1.retention_time,\n            t1.last_altered\n    FROM snowflake.account_usage.table_storage_metrics t\n        JOIN snowflake.account_usage.tables t1\n          ON t.id=t1.table_id\n    WHERE 1=1\n        AND t1.table_catalog not in ('SNOWFLAKE')\n        AND (churn_pct>=40 OR table_life_duration_hours<=24)\n    ORDER BY total_size_gb desc\n\"\"\")\n\nprint(\"ë°ì´í„°ë² ì´ìŠ¤ë³„ í…Œì´ë¸” ìˆ˜\")\nfig_db = px.bar(HighChurn_ShortLived_df, x='DATABASE', color='DATABASE', title='ë°ì´í„°ë² ì´ìŠ¤ë³„ í…Œì´ë¸” ìˆ˜')\nfig_db.show()\n\nprint(\"ìŠ¤í‚¤ë§ˆë³„ í…Œì´ë¸” ìˆ˜\")\nfig_schema = px.bar(HighChurn_ShortLived_df, x='SCHEMA', color='SCHEMA', title='ìŠ¤í‚¤ë§ˆë³„ í…Œì´ë¸” ìˆ˜')\nfig_schema.show()\n\nprint(\"í…Œì´ë¸” ì •ë³´\")\nfig_table = go.Figure(data=[go.Table(\n    header=dict(values=['DATABASE', 'SCHEMA', 'TABLE_NAME', 'DELETED', 'IS_TRANSIENT', 'LAST_ALTERED'], align='left'),\n    cells=dict(values=[HighChurn_ShortLived_df['DATABASE'], HighChurn_ShortLived_df['SCHEMA'], \n               HighChurn_ShortLived_df['TABLE_NAME'], HighChurn_ShortLived_df['DELETED'],\n               HighChurn_ShortLived_df['IS_TRANSIENT'], HighChurn_ShortLived_df['LAST_ALTERED']], align='left'))\n])\nfig_table.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d404fb1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": "title('ì§€ë‚œ 7ì¼ê°„ ì¿¼ë¦¬ì—ì„œ ì‚¬ìš©ëœ í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸')\n\nTablesUsedInQuery_df = run_sql(\"\"\"\n    WITH access_history as (   \n        SELECT distinct split(base.value:objectName, '.')[0]::string as DATABASE,\n               split(base.value:objectName, '.')[1]::string as SCHEMA,\n               split(base.value:objectName, '.')[2]::string as TABLE_NAME\n        FROM snowflake.account_usage.access_history,\n             lateral flatten (base_objects_accessed) base\n        WHERE query_start_time between current_date()-7 and current_date()\n    )\n    SELECT tbl.table_catalog||'.'||tbl.table_schema||'.'||tbl.table_name as FQ_table_name,\n           ah.database,\n           ah.schema,\n           ah.table_name,\n           TO_NUMBER(tbl.bytes / POWER(1024,3),10,2) AS GB\n    FROM snowflake.account_usage.tables tbl\n    LEFT JOIN access_history ah\n      ON tbl.table_name=ah.table_name\n     AND tbl.table_schema=ah.schema\n     AND tbl.table_catalog=ah.database\n    WHERE ah.table_name is not NULL\n      AND tbl.deleted is null\n\"\"\")\n\ndf_sorted = TablesUsedInQuery_df.sort_values(by='GB', ascending=False)\nfig_size = px.bar(df_sorted, y='DATABASE', x='GB', color='TABLE_NAME',\n                 title='í…Œì´ë¸”ë³„ ì´ í¬ê¸° (GB)', orientation='h')\nfig_size.show()\n\nfig_table = go.Figure(data=[go.Table(\n    header=dict(values=['DATABASE', 'SCHEMA', 'TABLE_NAME', 'GB'], align='left'),\n    cells=dict(values=[df_sorted['DATABASE'], df_sorted['SCHEMA'], \n               df_sorted['TABLE_NAME'], df_sorted['GB']], align='left'))\n])\nfig_table.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}