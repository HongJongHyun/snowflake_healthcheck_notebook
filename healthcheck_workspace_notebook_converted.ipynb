{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Snowflake Account Usage - Workspace Version\n\n### ì´ ì•±ì€ ì—¬ëŸ¬ë¶„ì˜ Snowflake ê³„ì •ì— ìˆëŠ” account_usage ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.\n\nì´ ë…¸íŠ¸ë¶ì€ Snowflake Notebooksì—ì„œ VS Code Workspaceìš©ìœ¼ë¡œ ë³€í™˜ëœ ë²„ì „ì…ë‹ˆë‹¤.\nSQL ì…€ì˜ ê²°ê³¼ëŠ” ìë™ìœ¼ë¡œ dataframe_X í˜•íƒœë¡œ í• ë‹¹ë©ë‹ˆë‹¤.\n\nìì„¸í•œ ì •ë³´ëŠ” ì•„ë˜ Snowflake ê³µì‹ ë¬¸ì„œ í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.  \nhttps://docs.snowflake.com/en/sql-reference/account-usage#account-usage-views",
      "id": "cell-0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# íŒ¨í‚¤ì§€ ì„¤ì • ë° ì¸¡ì • ë‚ ì§œ ì§€ì •",
      "id": "cell-1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "import pandas as pd\nimport plotly.express as px\nimport datetime\nimport altair as alt\nimport plotly.graph_objects as go\nimport numpy as np\n\n#alt.data_transformers.enable(\"vegafusion\")\n#alt.renderers.enable(\"mimetype\")\n    \nfrom IPython.display import display, Markdown\n\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\ndef title(text):\n    display(Markdown(f\"## {text}\"))",
      "id": "cell-2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ëª¨ë‹ˆí„°ë§ ë‚ ì§œ ë²”ìœ„ ì§€ì •')\n\nDAYS = 30\n\ntoday = datetime.date.today()\ns = (today - datetime.timedelta(days=DAYS)).strftime('%Y-%m-%d')\ne = today.strftime('%Y-%m-%d')\n\nprint(f\"ë¶„ì„ ê¸°ê°„: {s} ~ {e}\")\nprint(f\"\\nâ€» ê¸°ê°„ ë³€ê²½í•˜ë ¤ë©´ ìœ„ì˜ DAYS ê°’ì„ ìˆ˜ì •í•˜ì„¸ìš” (7, 14, 30, 60, 120)\")",
      "id": "cell-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# ì‚¬ìš©ëŸ‰ ê°œìš”",
      "id": "cell-4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_1"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_1\n--Credits Used\nselect round(sum(credits_used),0) as total_credits \n  from snowflake.account_usage.metering_history \n where start_time between '{{s}}' and '{{e}}' ;",
      "id": "cell-5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_2"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_2\n--Total # of Jobs Executed\nselect count(*) as number_of_jobs \n  from snowflake.account_usage.query_history \n where start_time between '{{s}}' and '{{e}}' ;",
      "id": "cell-6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_3"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_3\n--Current Storage\nselect round(avg(storage_bytes + stage_bytes + failsafe_bytes) / power(1024, 4),2) as billable_tb \n  from snowflake.account_usage.storage_usage \n where USAGE_DATE = current_date() - 1;",
      "id": "cell-7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('Account ì‚¬ìš©ëŸ‰ ê°œìš”')\n\npandas_credits_used_df = dataframe_1\npandas_num_jobs_df = dataframe_2\npandas_current_storage_df = dataframe_3\n\ncredits_used_tile = pandas_credits_used_df.iloc[0].values[0]\nnum_jobs_tile = pandas_num_jobs_df.iloc[0].values[0]\ncurrent_storage_tile = pandas_current_storage_df.iloc[0].values[0]\n\nprint(f\"ğŸ“Š Credits Used: {int(credits_used_tile):,}\")\nprint(f\"ğŸ“Š Total # of Jobs Executed: {int(num_jobs_tile):,}\")\nprint(f\"ğŸ“Š Current Storage (TB): {current_storage_tile}\")",
      "id": "cell-8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_4"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_4\n--Credits Billed by Month \nselect date_trunc('MONTH', usage_date) as Usage_Month, \n       sum(CREDITS_BILLED) as sum_credits\n  from snowflake.account_usage.metering_daily_history \n WHERE usage_date >= DATEADD('MONTH', -12, DATE_TRUNC('MONTH', CURRENT_TIMESTAMP()))\n group by Usage_Month ;",
      "id": "cell-9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì›”ë³„ ì „ì²´ í¬ë ˆë”§ ì‚¬ìš©ëŸ‰ ì¶”ì´')\n\ncredits_billed_df = dataframe_4\nfig_credits_billed=px.bar(credits_billed_df,x='USAGE_MONTH',y='SUM_CREDITS', orientation='v',title=\"Credits Billed by Month\")\nfig_credits_billed.show()\n\nprint('The above chart is static and not modified by the date range filter')",
      "id": "cell-10"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_5"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_5\n--Total # of Jobs Executed by Month \nSELECT DATE_TRUNC('MONTH', START_TIME) AS job_month, \n       COUNT(*) AS number_of_jobs                   \n  FROM snowflake.account_usage.query_history       \n WHERE START_TIME >= DATEADD('MONTH', -12, DATE_TRUNC('MONTH', CURRENT_TIMESTAMP()))        \n GROUP BY 1                                            \n ORDER BY 1 ASC                                        \n;",
      "id": "cell-11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì›”ë³„ ì „ì²´ Job ìˆ˜í–‰ ìˆ˜ ë³€í™” ì¶”ì´')\n\nJobByMonth_df = dataframe_5\nfig_JobByMonth=px.bar(JobByMonth_df,x='JOB_MONTH',y='NUMBER_OF_JOBS', orientation='v',title=\"Jobs by Month\")\nfig_JobByMonth.show()\n\nprint('The above chart is static and not modified by the date range filter')",
      "id": "cell-12"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# ê°€ìƒ ì›¨ì–´í•˜ìš°ìŠ¤ (Virtual Warehouse) ë¶„ì„",
      "id": "cell-13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_6"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_6\n-- Credits Usages of Warehouse\nselect warehouse_name,sum(credits_used) as total_credits_used \n  from snowflake.account_usage.warehouse_metering_history \n where start_time between '{{s}}' and '{{e}}' \n group by 1 \n order by 2 desc \n limit 10 ;",
      "id": "cell-14"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ ì‚¬ìš©ëœ í¬ë ˆë”§')\n\npandas_credits_used_df = dataframe_6\npandas_credits_used_df_sorted = pandas_credits_used_df.sort_values(by='TOTAL_CREDITS_USED', ascending=True)\n\nfig_credits_used=px.bar(pandas_credits_used_df_sorted,x='TOTAL_CREDITS_USED',y='WAREHOUSE_NAME',orientation='h',title=\"Credits Used by Warehouse\")\nfig_credits_used.update_traces(marker_color='green')\nfig_credits_used.show()",
      "id": "cell-15"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_7"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_7\n--Jobs by Warehouse Data Setup\nselect warehouse_name,count(*) as number_of_jobs \n  from snowflake.account_usage.query_history \n where start_time between '{{s}}' and '{{e}}' \n   and warehouse_name is not null\n group by 1 \n order by 2 desc \n limit 10 ;",
      "id": "cell-16"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ ìˆ˜í–‰ëœ ì‘ì—…ì˜ ìˆ˜')\n\npandas_jobs_by_warehouse_df = dataframe_7\npandas_jobs_by_warehouse_df_sorted = pandas_jobs_by_warehouse_df.sort_values(by='NUMBER_OF_JOBS', ascending=True)\n\nfig_jobs_by_warehouse=px.bar(pandas_jobs_by_warehouse_df_sorted,x='NUMBER_OF_JOBS',y='WAREHOUSE_NAME',orientation='h',title=\"# of Jobs by Warehouse\")\nfig_jobs_by_warehouse.update_traces(marker_color='purple')\nfig_jobs_by_warehouse.show()",
      "id": "cell-17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_8"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_8\n--Average Execution by Query Type\nselect query_type, warehouse_size, avg(execution_time) / 1000 as average_execution_time \n  from snowflake.account_usage.query_history \n where start_time between '{{s}}' and '{{e}}' \n group by 1, 2 \n order by 3 desc;",
      "id": "cell-18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ ì¿¼ë¦¬ ìœ í˜•ë³„, ì›¨ì–´í•˜ìš°ìŠ¤ í¬ê¸°ë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')\n\npandas_execution_by_qtype_df = dataframe_8\nfig_execution_by_qtype=px.bar(pandas_execution_by_qtype_df,x='AVERAGE_EXECUTION_TIME',y='QUERY_TYPE',color='WAREHOUSE_SIZE',orientation='h',title=\"Average Execution Time by Query Type and Warehouse Size\")\nfig_execution_by_qtype.show()",
      "id": "cell-19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_9"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_9\n--Credits Used Overtime\nselect start_time::date as usage_date, warehouse_name, sum(credits_used) as total_credits_used \n  from snowflake.account_usage.warehouse_metering_history \n where start_time between '{{s}}' and '{{e}}' \n group by 1,2 \nhaving total_credits_used > 0.001\n order by 2,1 ;",
      "id": "cell-20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ í¬ë ˆë”§ ì‚¬ìš©ì–‘ ì¶”ì„¸')\n\npandas_credits_used_overtime_df = dataframe_9\nfig_credits_used_overtime_df=px.bar(pandas_credits_used_overtime_df,x='USAGE_DATE',y='TOTAL_CREDITS_USED',color='WAREHOUSE_NAME',orientation='v',title=\"Credits Used Overtime\")\nfig_credits_used_overtime_df.show()",
      "id": "cell-21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_10"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_10\n--Warehouse Variance Overtime\nSELECT WAREHOUSE_NAME, \n       DATE(START_TIME) AS DATE, \n       SUM(CREDITS_USED) AS CREDITS_USED, \n       AVG(SUM(CREDITS_USED)) OVER (PARTITION BY WAREHOUSE_NAME ORDER BY DATE ROWS 7 PRECEDING) \n         AS CREDITS_USED_7_DAY_AVG, \n       (TO_NUMERIC(SUM(CREDITS_USED)/CREDITS_USED_7_DAY_AVG*100,10,2)-100)::STRING || '%' \n         AS VARIANCE_TO_7_DAY_AVERAGE \n  FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY \n where start_time between '{{s}}' and '{{e}}' \n GROUP BY DATE, WAREHOUSE_NAME \n ORDER BY DATE DESC",
      "id": "cell-22"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë‚´ì˜ í•´ë‹¹ ë‚ ì§œë³„ ì´ì „ 7ì¼ê°„ì˜ í‰ê·  ì›¨ì–´í•˜ìš°ìŠ¤ ì‚¬ìš©ëŸ‰ ëŒ€ë¹„ ì¦ê°ì–‘')\n\npandas_warehouse_variance_df = dataframe_10\nfig_warehouse_variance_df=px.bar(pandas_warehouse_variance_df,x=\"DATE\",y=\"VARIANCE_TO_7_DAY_AVERAGE\",color ='WAREHOUSE_NAME',orientation='v',title=\"Warehouse Variance Greater than 7 day Average\")\nfig_warehouse_variance_df.show()",
      "id": "cell-23"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_11"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_11\nwith base_table as (\nselect warehouse_name, \n       total_elapsed_time,\n       case\n            when total_elapsed_time < 1 then 'a'\n            when total_elapsed_time < 10 then 'b'\n            when total_elapsed_time < 60 then 'c'\n            when total_elapsed_time < 300 then 'd'\n            when total_elapsed_time < 600 then 'e'\n            when total_elapsed_time > 600 then 'f'\n       end as exe_time\n  from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\n where warehouse_name is not null\n   and start_time between '{{s}}' and '{{e}}'\n   and warehouse_name not ilike 'compute_service%'   \n)\nselect warehouse_name, \n       sum(iff(exe_time= 'a', 1, 0)) u00_01s,\n       sum(iff(exe_time= 'b', 1, 0)) u01_10s,\n       sum(iff(exe_time= 'c', 1, 0)) u10_60s,\n       sum(iff(exe_time= 'd', 1, 0)) u01_05m,\n       sum(iff(exe_time= 'e', 1, 0)) u05_10m,\n       sum(iff(exe_time= 'f', 1, 0)) u10m_\n  from base_table\n group by warehouse_name;",
      "id": "cell-24"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ ë™ì•ˆ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ ì›Œí¬ë¡œë“œ ìˆ˜í–‰ì‹œê°„ ë¶„í¬')\n\nExeTimeInWH_df = dataframe_11\n\nplot_columns = ['U00_01S', 'U01_10S', 'U10_60S', 'U01_05M', 'U05_10M', 'U10M_']\n\nlabels_map = {\n    'WAREHOUSE_NAME': 'Warehouse Name',\n    'variable': 'Execution Time Bucket',\n    'value': 'Number of Queries',\n    'U00_01S': '< 1s',\n    'U01_10S': '1s - 10s',\n    'U10_60S': '10s - 60s',\n    'U01_05M': '1m - 5m',\n    'U05_10M': '5m - 10m',\n    'U10M_': '> 10m'\n}\n\ndf_long = pd.melt(ExeTimeInWH_df,\n                  id_vars='WAREHOUSE_NAME',\n                  value_vars=plot_columns,\n                  var_name='variable',\n                  value_name='value')\n\nfig = px.bar(\n    df_long,\n    x='WAREHOUSE_NAME',\n    y='value',\n    color='variable',\n    title='Query Execution Time Distribution by Warehouse',\n    labels=labels_map,\n    height=500,\n    category_orders={'variable': plot_columns}\n)\n\nfig.update_yaxes(tickformat=',')\nfig.show()",
      "id": "cell-25"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_12"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_12\nselect warehouse_name,\n       percentile_cont(0.25) within group(order by query_load_percent) as p25,\n       median(query_load_percent) as md,\n       percentile_cont(0.75) within group(order by query_load_percent) as p75\n  from snowflake.account_usage.query_history\n where warehouse_name is not null\n   and query_load_percent is not null\n   and start_time between '{{s}}' and '{{e}}'\n   and warehouse_name not ilike 'compute_service%'\n group by 1\n order by 3 desc ;",
      "id": "cell-26"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ ì›Œí¬ë¡œë“œ ì‚¬ìš©ëŸ‰ ë¶„í¬ (25%, 50%, 75%)')\n\nWHWorkloadPCT_df = dataframe_12\n\nWHWorkloadPCT_df['error_y_upper'] = WHWorkloadPCT_df['P75'] - WHWorkloadPCT_df['MD']\nWHWorkloadPCT_df['error_y_lower'] = WHWorkloadPCT_df['MD']  - WHWorkloadPCT_df['P25']\n\nfig = px.scatter(\n    WHWorkloadPCT_df,\n    x='WAREHOUSE_NAME',\n    y='MD',\n    error_y='error_y_upper',\n    error_y_minus='error_y_lower',\n    title='Warehouse Query Load Distribution (Median and P25-P75 Range)',\n    labels={\n        'WAREHOUSE_NAME': 'Warehouse',\n        'MD': 'Median Query Load (%)',\n        'P25': 'P25 Load (%)',\n        'P75': 'P75 Load (%)'\n    },\n    hover_data=['P25', 'MD', 'P75']\n)\n\nfig.update_layout(\n    yaxis_title='Query Load Percent (%)',\n    xaxis_title='Warehouse Name',\n    yaxis_range=[0, max(100, WHWorkloadPCT_df['P75'].max() * 1.1)],\n    yaxis_ticksuffix='%'\n)\n\nfig.update_traces(\n    error_y_thickness=1,\n    error_y_width=5\n)\n\nfig.show()",
      "id": "cell-27"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_13"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_13\nSELECT to_date(start_time) as date,\n       warehouse_name,\n       sum(avg_running) as sum_running,\n       sum(avg_queued_load) as sum_queued\n  FROM snowflake.account_usage.warehouse_load_history\n WHERE to_date(start_time) >= dateadd(month, -1, current_timestamp())\n GROUP BY 1,2\n ORDER BY 1,2 ;",
      "id": "cell-28"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì¼ë³„ ì›¨ì–´í•˜ìš°ìŠ¤ì˜ ì›Œí¬ë¡œë“œ ì¶”ì´ (ì§€ë‚œ 1ê°œì›”)')\n\nRunQueueWH_df = dataframe_13\n\nfig_running = px.line(\n    RunQueueWH_df,\n    x='DATE',\n    y='SUM_RUNNING',\n    color='WAREHOUSE_NAME',\n    title='Daily Sum of Average Running Queries by Warehouse',\n    labels={\n        'DATE': 'Date',\n        'SUM_RUNNING': 'Sum of Avg Running Queries (Daily)',\n        'WAREHOUSE_NAME': 'Warehouse'\n    },\n    markers=True,\n    hover_data={'WAREHOUSE_NAME': True, 'SUM_RUNNING': ':.1f'}\n)\nfig_running.update_layout(\n    xaxis_title='Date',\n    yaxis_title='Daily Sum of Avg Running Queries',\n    hovermode='x unified'\n)\nfig_running.show()\n\nfig_queued = px.line(\n    RunQueueWH_df,\n    x='DATE',\n    y='SUM_QUEUED',\n    color='WAREHOUSE_NAME',\n    title='Daily Sum of Average Queued Queries by Warehouse',\n    labels={\n        'DATE': 'Date',\n        'SUM_QUEUED': 'Sum of Avg Queued Queries (Daily)',\n        'WAREHOUSE_NAME': 'Warehouse'\n    },\n    markers=True,\n    hover_data={'WAREHOUSE_NAME': True, 'SUM_QUEUED': ':.1f'}\n)\nfig_queued.update_layout(\n    xaxis_title='Date',\n    yaxis_title='Daily Sum of Avg Queued Queries',\n    hovermode='x unified'\n)\nmin_queued = RunQueueWH_df['SUM_QUEUED'].min()\nmax_queued = RunQueueWH_df['SUM_QUEUED'].max()\nfig_queued.update_yaxes(range=[min_queued, max(1, max_queued * 1.1)])\nfig_queued.show()",
      "id": "cell-29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# ì¿¼ë¦¬ ë¶„ì„",
      "id": "cell-30"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_14"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_14\n--Top 25 Longest Success Queries\nselect query_id,query_text,(execution_time / 1000) as exec_time \n  from snowflake.account_usage.query_history \n where execution_status = 'SUCCESS' \n   and start_time between '{{s}}' and '{{e}}' \n   and query_text != ''\n   and exec_time > 10\n order by execution_time desc \n limit 25;",
      "id": "cell-31"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ê°€ì¥ ì˜¤ë˜ ìˆ˜í–‰ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸')\n\npandas_longest_queries_df = dataframe_14\n\nmax_len = 100\ncol_orig = 'QUERY_TEXT'\ncol_short = 'QUERY_TEXT_SHORT'\n\npandas_longest_queries_df[col_short] = pandas_longest_queries_df[col_orig].str.slice(0, max_len)\npandas_longest_queries_df.loc[pandas_longest_queries_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_longest_queries = px.bar(\n    pandas_longest_queries_df,\n    x='EXEC_TIME',\n    y=col_short,\n    orientation='h',\n    title=\"Longest Successful Queries (Top 25)\",\n    hover_data={\n        'EXEC_TIME': ':.2f s',\n        col_orig: True,\n        col_short: False\n    },\n    labels={col_short: \"Query Text (Truncated)\"}\n)\n\nfig_longest_queries.show()",
      "id": "cell-32"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_15"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_15\n--Top 25 Failed Queries \nselect query_text,count(*) as number_of_execution  \n  from snowflake.account_usage.query_history \n where execution_status = 'FAIL' \n   and start_time between '{{s}}' and '{{e}}' \n   and query_text != ''\n group by 1\nhaving number_of_execution > 1\n order by 2 desc \n limit 25;",
      "id": "cell-33"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ê°€ì¥ ë§ì´ ì‹¤íŒ¨í•œ ì¿¼ë¦¬ ë¬¸ ë¦¬ìŠ¤íŠ¸')\n\nf_pandas_longest_queries_df = dataframe_15\n\nmax_len = 100\ncol_orig = 'QUERY_TEXT'\ncol_short = 'QUERY_TEXT_SHORT'\n\nf_pandas_longest_queries_df[col_short] = f_pandas_longest_queries_df[col_orig].str.slice(0, max_len)\nf_pandas_longest_queries_df.loc[f_pandas_longest_queries_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_f_longest_queries = px.bar(\n    f_pandas_longest_queries_df,\n    x='NUMBER_OF_EXECUTION',\n    y=col_short,\n    orientation='h',\n    title=\"Most Failed Queries (Top 25)\",\n    hover_data={\n        'NUMBER_OF_EXECUTION': ':3d s', \n        col_orig: True,\n        col_short: False\n    },\n    labels={col_short: \"Query Text (Truncated)\"}\n)\nfig_f_longest_queries.update_traces(marker_color='red')\n\nfig_f_longest_queries.show()",
      "id": "cell-34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_16"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_16\n--Total Execution Time by Repeated Queries\nselect query_text, \n       (sum(execution_time) / 1000) as exec_time,\n       count(*) as number_of_query\n  from snowflake.account_usage.query_history \n where execution_status = 'SUCCESS' \n   and start_time between '{{s}}' and '{{e}}' \n   and query_text != ''\n group by query_text \n order by exec_time desc \n limit 10 ;",
      "id": "cell-35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë‚´ì˜ ë°˜ë³µë˜ëŠ” ì¿¼ë¦¬ ìˆ˜í–‰')\n\nTotal_Execution_Time_df = dataframe_16\n\nmax_len = 100\ncol_orig = 'QUERY_TEXT'\ncol_short = 'QUERY_TEXT_SHORT'\n\nTotal_Execution_Time_df[col_short] = Total_Execution_Time_df[col_orig].str.slice(0, max_len)\nTotal_Execution_Time_df.loc[Total_Execution_Time_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_Total_Execution_Time = px.bar(\n    Total_Execution_Time_df,\n    x='EXEC_TIME',\n    y=col_short,\n    orientation='h',\n    title=\"Total Execution Time by Repeated Queries\",\n    hover_data={\n        'EXEC_TIME': ':.3f s',\n        col_orig: True,\n        col_short: False,\n        'NUMBER_OF_QUERY': ':,d'\n    },\n    labels={\n        col_short: \"Query Text (Truncated)\",\n        'EXEC_TIME': \"Total Execution Time (s)\",\n        'NUMBER_OF_QUERY': \"Number of Executions\"\n    }\n)\nfig_Total_Execution_Time.update_traces(marker_color='LightSkyBlue')\n\nfig_Total_Execution_Time.show()",
      "id": "cell-36"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_17"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_17\n--QUERIES THAT SPILL TO DISK\nselect top 25\n       query_text\n      ,avg(total_elapsed_time) as avg_total_elapsed_time\n      ,sum(bytes_spilled_to_local_storage) / 1024 / 1024 / 1024 as sum_gb_spilled_to_disk\n      ,sum(bytes_spilled_to_remote_storage) / 1024 / 1024 / 1024 as sum_gb_spilled_to_blob\n      ,sum(bytes_spilled_to_local_storage+bytes_spilled_to_remote_storage) as total_bytes\n  from snowflake.account_usage.query_history\n where start_time between '{{s}}' and '{{e}}' \n   and query_text != ''\n group by query_text\nhaving total_bytes > 0\n order by total_bytes desc;",
      "id": "cell-37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ ë™ì•ˆ Diskë¡œ Spillëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸')\n\nSpillToDisk_df = dataframe_17\n\nmax_len = 100\ncol_orig = 'QUERY_TEXT'\ncol_short = 'QUERY_TEXT_SHORT'\n\nSpillToDisk_df[col_short] = SpillToDisk_df[col_orig].str.slice(0, max_len)\nSpillToDisk_df.loc[SpillToDisk_df[col_orig].str.len() > max_len, col_short] += '...'\n\nfig_SpillToDisk = px.bar(\n    SpillToDisk_df,\n    x='TOTAL_BYTES',\n    y=col_short,\n    orientation='h',\n    title=\"Spill to Disk Queries\",\n    hover_data={\n        'AVG_TOTAL_ELAPSED_TIME': ':.2f',  \n        'TOTAL_BYTES': ':.d',  \n        col_orig: True,\n        col_short: False\n    },\n    labels={\n        col_short: \"Query Text (Truncated)\",\n        'TOTAL_BYTES': \"Spill to Disk\",\n        'AVG_TOTAL_ELAPSED_TIME': \"Average Elapsed Time\"\n    }\n)\nfig_SpillToDisk.update_traces(marker_color='Yellow')\n\nfig_SpillToDisk.show()",
      "id": "cell-38"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_18"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_18\n--Top 10 Average Query Execution Time (By User)\nselect user_name, (avg(execution_time)) / 1000 as average_execution_time \n  from snowflake.account_usage.query_history \n where execution_status = 'SUCCESS'\n   and start_time between '{{s}}' and '{{e}}'\n group by 1 \n order by 2 desc limit 10 ;",
      "id": "cell-39"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ì‚¬ìš©ìë³„ í‰ê·  ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')\n\nquery_execution_df = dataframe_18\nfig_cquery_execution=px.bar(query_execution_df,x='USER_NAME',y='AVERAGE_EXECUTION_TIME', orientation='v',title=\"Average Execution Time per User\")\nfig_cquery_execution.update_traces(marker_color='MediumPurple')\nfig_cquery_execution.show()",
      "id": "cell-40"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_19"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_19\nSELECT query_history.query_id,\n       query_history.query_text,\n       query_history.start_time,\n       query_history.end_time,\n       query_history.user_name,\n       query_history.database_name,\n       query_history.schema_name,\n       query_history.warehouse_name,\n       query_history.warehouse_size,\n       metering_history.credits_used,\n       execution_time/1000 as execution_time_s\n  FROM snowflake.account_usage.query_history\n  JOIN snowflake.account_usage.metering_history \n    ON query_history.start_time >= metering_history.start_time\n   AND query_history.end_time <= metering_history.end_time\n WHERE query_history.start_time >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())\n ORDER BY query_history.query_id;",
      "id": "cell-41"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('í•œ ì£¼ê°„ ì‹œê°„ëŒ€ë³„ ì¿¼ë¦¬ ì‚¬ìš©ëŸ‰ - Heatmap')\n\ndf = dataframe_19\n\ndays = 7 \nvar = 'WAREHOUSE_NAME'\n\ndf['START_TIME'] = pd.to_datetime(df['START_TIME']) \nlatest_date = df['START_TIME'].max() \ncutoff_date = latest_date - pd.Timedelta(days=days) \nfiltered_df = df[df['START_TIME'] > cutoff_date].copy()\n\nfiltered_df['HOUR_OF_DAY'] = filtered_df['START_TIME'].dt.hour \nfiltered_df['HOUR_DISPLAY'] = filtered_df['HOUR_OF_DAY'].apply(lambda x: f\"{x:02d}:00\")\n\nagg_df = (filtered_df.groupby(['HOUR_DISPLAY', var]) .agg(COUNT=('QUERY_ID', 'count')) .reset_index() )\n\npivot_df = agg_df.pivot(index=var, columns='HOUR_DISPLAY', values='COUNT').fillna(0)\n\nprint(f\"Analyzing {var} data for the last {days} days!\")\n\nfig = px.imshow( pivot_df, labels=dict(x=\"Hour\", y=\"Warehouse\", color=\"Query Count\"), title=f'Query Activity Heatmap by Hour and {var}', aspect=\"auto\", color_continuous_scale=\"Blues\" ) \nfig.update_layout(height=max(400, len(pivot_df) * 30)) \nfig.show()",
      "id": "cell-42"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_20"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_20\n--GS Utilization by Query Type (Top 10)\nselect query_type, \n       sum(credits_used_cloud_services) cs_credits, \n       count(1) num_queries \n  from snowflake.account_usage.query_history \n where true \n   and start_time between '{{s}}' and '{{e}}'\n group by 1 \n order by 2 desc \n limit 10 ;",
      "id": "cell-44"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ì¿¼ë¦¬ ìœ í˜•ë³„ í´ë¼ìš°ë“œì„œë¹„ìŠ¤ ì´ ì‚¬ìš© í¬ë ˆë”§ëŸ‰')\n\ngs_utilization_df = dataframe_20\n\nfig_gs_utilization = px.bar(\n    gs_utilization_df,\n    x='QUERY_TYPE',\n    y='CS_CREDITS',\n    orientation='v',\n    title=\"GS Utilization by Query Type (Top 10)\",\n    hover_data={\n        'QUERY_TYPE': False,\n        'CS_CREDITS': ':.2f credits',\n        'NUM_QUERIES': ':,d'\n    },\n    labels={\n        'CS_CREDITS': 'Cloud Service Credits',\n        'NUM_QUERIES': 'Number of Queries'\n    }\n)\nfig_gs_utilization.update_traces(marker_color='green')\nfig_gs_utilization.show()",
      "id": "cell-45"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_21"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_21\n--Top 10 Cloud Services by Warehouse   \nselect warehouse_name, \n       sum(credits_used_cloud_services) CREDITS_USED_CLOUD_SERVICES \n  from snowflake.account_usage.warehouse_metering_history \n where true \n   and start_time between '{{s}}' and '{{e}}'\n group by 1 \n order by 2 desc limit 10;",
      "id": "cell-46"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ì›¨ì–´í•˜ìš°ìŠ¤ë³„ í´ë¼ìš°ë“œì„œë¹„ìŠ¤ì˜ ì´ ì‚¬ìš© í¬ë ˆë”§ì–‘')\n\ncompute_gs_by_warehouse_df = dataframe_21\nfig_compute_gs_by_warehouse=px.bar(compute_gs_by_warehouse_df,x='WAREHOUSE_NAME',y='CREDITS_USED_CLOUD_SERVICES', orientation='v',title=\"Compute and Cloud Services by Warehouse\", barmode=\"group\")\nfig_compute_gs_by_warehouse.update_traces(marker_color='purple')\nfig_compute_gs_by_warehouse.show()",
      "id": "cell-47"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# ìŠ¤í† ë¦¬ì§€",
      "id": "cell-48"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_22"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_22\n--Data Storage used Overtime \nselect date_trunc(month, usage_date) as usage_month, \n       avg(storage_bytes + stage_bytes + failsafe_bytes) / power(1024, 4) as billable_tb, \n       avg(storage_bytes) / power(1024, 4) as Storage_TB, \n       avg(stage_bytes) / power(1024, 4) as Stage_TB, \n       avg(failsafe_bytes) / power(1024, 4) as Failsafe_TB \n  from snowflake.account_usage.storage_usage \n group by 1 \n order by 1 ;",
      "id": "cell-49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì›”ë³„ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ì¶”ì´')\n\nstorage_overtime_df = dataframe_22\n\nstorage_df_melted = pd.melt(\n    storage_overtime_df,\n    id_vars=['USAGE_MONTH'],\n    value_vars=['STORAGE_TB', 'STAGE_TB', 'FAILSAFE_TB'],\n    var_name='STORAGE_TYPE',\n    value_name='SIZE_TB'\n)\n\nstorage_type_order = ['STORAGE_TB', 'STAGE_TB', 'FAILSAFE_TB']\nstorage_df_melted['STORAGE_TYPE'] = pd.Categorical(\n    storage_df_melted['STORAGE_TYPE'], categories=storage_type_order, ordered=True\n)\nstorage_df_melted = storage_df_melted.sort_values(by=['USAGE_MONTH', 'STORAGE_TYPE'])\n\nfig_stacked_storage = px.bar(\n    storage_df_melted,\n    x='USAGE_MONTH',\n    y='SIZE_TB',\n    color='STORAGE_TYPE',\n    title=\"Data Storage used Overtime\",\n    orientation='v',\n    hover_data={\n        'USAGE_MONTH': True,\n        'STORAGE_TYPE': True,\n        'SIZE_TB': ':.2f TB'\n    },\n    labels={\n        'USAGE_MONTH': 'Month',\n        'SIZE_TB': 'Storage Size (TB)',\n        'STORAGE_TYPE': 'Storage Type'\n    }\n)\n\nfig_stacked_storage.show()\nprint('The above chart is static and not modified by the date range filter')",
      "id": "cell-50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_23"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_23\n--Rows Loaded Overtime (COPY INTO) \nselect to_timestamp(date_trunc(day,last_load_time)) as usage_date, \n       sum(row_count) as total_rows \n  from snowflake.account_usage.load_history \n where usage_date between '{{s}}' and '{{e}}' \n group by 1 \n order by usage_date desc ;",
      "id": "cell-51"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ì¼ë³„ ë°ì´í„° ì ì¬ê±´ìˆ˜ ì¶”ì´')\n\nrows_loaded_df = dataframe_23\n\nfig_rows_loaded=px.bar(rows_loaded_df,x='USAGE_DATE',y='TOTAL_ROWS', orientation='v',title=\"Rows Loaded Overtime (Copy Into)\")\nfig_rows_loaded.show()",
      "id": "cell-52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# ì‚¬ìš©ì",
      "id": "cell-53"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_24"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_24\n--Logins by User \nselect user_name, \n       sum(iff(is_success = 'NO', 1, 0)) as Failed, \n       count(*) as Success, \n       sum(iff(is_success = 'NO', 1, 0)) / nullif(count(*), 0) as login_failure_rate \n  from snowflake.account_usage.login_history \n where true\n   and event_timestamp between '{{s}}' and '{{e}}'\n group by 1 \n order by 4 desc;",
      "id": "cell-54"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ ì‚¬ìš©ìë³„ ë¡œê·¸ì¸ ì„±ê³µ/ì‹¤íŒ¨ íšŸìˆ˜')\n\nlogins_df_wide = dataframe_24\n\nlogins_df_melted = pd.melt(\n    logins_df_wide,\n    id_vars=['USER_NAME'],\n    value_vars=['SUCCESS', 'FAILED'],\n    var_name='LOGIN_STATUS',\n    value_name='COUNT'\n)\n\nstatus_order = ['SUCCESS', 'FAILED']\nlogins_df_melted['LOGIN_STATUS'] = pd.Categorical(\n    logins_df_melted['LOGIN_STATUS'], categories=status_order, ordered=True\n)\nlogins_df_melted = logins_df_melted.sort_values(by=['USER_NAME', 'LOGIN_STATUS'])\n\nfig_logins_stacked = px.bar(\n    logins_df_melted,\n    x='USER_NAME',\n    y='COUNT',\n    color='LOGIN_STATUS',\n    title=\"Sucessful & Failed login by Users\",\n    orientation='v',\n    hover_data={\n        'USER_NAME': True,\n        'LOGIN_STATUS': True,\n        'COUNT': ':,d'\n    },\n    labels={\n        'USER_NAME': 'User Name',\n        'COUNT': 'Login Count',\n        'LOGIN_STATUS': 'Login Status'\n    },\n    color_discrete_map={\n        'SUCCESS': 'lightgreen',\n        'FAILED': 'red'\n    }\n)\n\nfig_logins_stacked.show()",
      "id": "cell-55"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_25"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_25\n--Logins by Client \nselect reported_client_type as Client, \n       user_name, \n       sum(iff(is_success = 'NO', 1, 0)) as Failed, \n       count(*) as Success, \n       sum(iff(is_success = 'NO', 1, 0)) / nullif(count(*), 0) as login_failure_rate \n  from snowflake.account_usage.login_history \n where true\n   and event_timestamp between '{{s}}' and '{{e}}'\n group by 1, 2 \n order by 5 desc ;",
      "id": "cell-56"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ì •ëœ ê¸°ê°„ë™ì•ˆ í´ë¼ì´ì–¸íŠ¸ ìœ í˜•ë³„, ì‚¬ìš©ìë³„ ë¡œê·¸ì¸ ì„±ê³µ/ì‹¤íŒ¨ íšŸìˆ˜')\n\nlogins_client_df_wide = dataframe_25\n\nlogins_client_df_melted = pd.melt(\n    logins_client_df_wide,\n    id_vars=['CLIENT', 'USER_NAME'],\n    value_vars=['SUCCESS', 'FAILED'],\n    var_name='LOGIN_STATUS',\n    value_name='COUNT'\n)\n\nstatus_order = ['SUCCESS', 'FAILED']\nlogins_client_df_melted['LOGIN_STATUS'] = pd.Categorical(\n    logins_client_df_melted['LOGIN_STATUS'], categories=status_order, ordered=True\n)\nlogins_client_df_melted = logins_client_df_melted.sort_values(by=['CLIENT', 'USER_NAME', 'LOGIN_STATUS'])\n\nfig_logins_client_grouped = px.bar(\n    logins_client_df_melted,\n    x='USER_NAME',\n    y='COUNT',\n    color='LOGIN_STATUS',\n    facet_col='CLIENT',\n    facet_col_wrap=3,\n    title=\"Logins by Client\",\n    hover_data={\n        'CLIENT': True,\n        'USER_NAME': True,\n        'LOGIN_STATUS': True,\n        'COUNT': ':,d'\n    },\n    labels={\n        'USER_NAME': 'Username',\n        'COUNT': 'Number of logins',\n        'LOGIN_STATUS': 'Login status',\n        'CLIENT': 'Client type'\n    },\n    color_discrete_map={\n        'SUCCESS': 'mediumseagreen',\n        'FAILED': 'tomato'\n    },\n    height=500\n)\n\nfig_logins_client_grouped.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\nfig_logins_client_grouped.update_xaxes(tickangle=45)\n\nfig_logins_client_grouped.show()",
      "id": "cell-57"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_26"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_26\n--Never Logged In Users\nSHOW USERS;",
      "id": "cell-58"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_27"
      },
      "outputs": [],
      "source": "SELECT * FROM {{dataframe_26}} \nWHERE last_success_login IS NULL\nAND DATEDIFF('Day',created_on,CURRENT_DATE) > 30;",
      "id": "cell-59"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ìƒì„± í›„ 30ì¼ ì´ë‚´ ë¡œê·¸ì¸í•œ ì´ë ¥ì´ ì—†ëŠ” ì‚¬ìš©ì')\n\ndf_users = dataframe_27\n\nif not df_users.empty:\n    df_users['CREATED_ON'] = pd.to_datetime(df_users['CREATED_ON'])\n\n    print(\"ì‚¬ìš©ì ìƒì„± íƒ€ì„ë¼ì¸\")\n\n    fig_timeline = px.scatter(\n        df_users, \n        x='CREATED_ON',     # ëŒ€ë¬¸ìë¡œ ë³€ê²½\n        y='NAME',           # ëŒ€ë¬¸ìë¡œ ë³€ê²½\n        title='Inactive User Creation Timeline',\n        labels={            \n            'CREATED_ON': 'Creation Date/Time',  # ëŒ€ë¬¸ìë¡œ ë³€ê²½\n            'NAME': 'User Name'                   # ëŒ€ë¬¸ìë¡œ ë³€ê²½\n        },\n        hover_name='NAME',  # ëŒ€ë¬¸ìë¡œ ë³€ê²½\n        hover_data={       \n            'CREATED_ON': '|%Y-%m-%d %H:%M:%S',  # ëŒ€ë¬¸ìë¡œ ë³€ê²½\n            'NAME': False                         # ëŒ€ë¬¸ìë¡œ ë³€ê²½\n        }\n    )\n\n    fig_timeline.update_layout(\n        xaxis_title='Creation Date/Time',\n        yaxis_title='User Name',\n        height=max(400, len(df_users['NAME'].unique()) * 20)  # ëŒ€ë¬¸ìë¡œ ë³€ê²½\n    )\n    fig_timeline.update_yaxes(categoryorder='category ascending')\n    fig_timeline.update_traces(marker=dict(size=8, symbol='circle'))\n\n    fig_timeline.show()\nelse:\n    print(\"ì¡°ê±´ì— ë§ëŠ” ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.\")",
      "id": "cell-60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_28"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_28\n--Stale Users\nSHOW USERS;",
      "id": "cell-61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_29"
      },
      "outputs": [],
      "source": "SELECT * FROM {{dataframe_28}}\nWHERE DATEDIFF('Day',last_success_login,CURRENT_DATE) > 30;",
      "id": "cell-62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ë‚œ 30ì¼ ì´ë‚´ ìƒˆë¡œ ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ì‚¬ìš©ì')\n\ndf_users = dataframe_29\n\nif not df_users.empty:\n    df_users['LAST_SUCCESS_LOGIN'] = pd.to_datetime(df_users['LAST_SUCCESS_LOGIN'])\n\n    print(\"ì‚¬ìš©ì ë§ˆì§€ë§‰ ë¡œê·¸ì¸ íƒ€ì„ë¼ì¸\")\n\n    fig_timeline = px.scatter(\n        df_users, \n        x='LAST_SUCCESS_LOGIN',     \n        y='NAME',           \n        title='Infrequent User',\n        labels={            \n            'LAST_SUCCESS_LOGIN': 'Last Success Login Date/Time',\n            'NAME': 'User Name'\n        },\n        hover_name='NAME',  \n        hover_data={       \n            'LAST_SUCCESS_LOGIN': '|%Y-%m-%d %H:%M:%S', \n            'NAME': False \n        }\n    )\n\n    fig_timeline.update_layout(\n        xaxis_title='Last Success Login Date/Time',\n        yaxis_title='User Name',\n        height=max(400, len(df_users['NAME'].unique()) * 20)\n    )\n    fig_timeline.update_yaxes(categoryorder='category ascending')\n    fig_timeline.update_traces(marker=dict(size=8, symbol='circle'))\n\n    fig_timeline.show()\nelse:\n    print(\"ì¡°ê±´ì— ë§ëŠ” ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.\")",
      "id": "cell-63"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# í…Œì´ë¸” ë¶„ì„",
      "id": "cell-64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_30"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_30\n-- Identify high churn tables or short lived tables\nSELECT\n        t.table_catalog||'.'||t.table_schema||'.'||t.table_name as fq_table_name\n       ,t.table_catalog as database\n       ,t.table_schema as schema\n       ,t.table_name as table_name\n       ,t.active_bytes/power(1024,3) as active_size_gb\n       ,t.time_travel_bytes/power(1024,3) as time_travel_gb\n       ,t.failsafe_bytes/power(1024,3) as failsafe_gb\n       ,t.retained_for_clone_bytes/power(1024,3) as clone_retain_gb\n       ,active_size_gb+time_travel_gb+failsafe_gb+clone_retain_gb as total_size_gb\n       ,(t.time_travel_bytes + t.failsafe_bytes + t.retained_for_clone_bytes)/power(1024,3) as non_active_size_gb\n       ,div0(non_active_size_gb,active_size_gb)*100 as churn_pct\n       ,t.deleted\n       ,timediff('hour',t.table_created,t.table_dropped) as table_life_duration_hours\n       ,t1.is_transient\n       ,t1.table_type\n       ,t1.retention_time\n       ,t1.auto_clustering_on\n       ,t1.clustering_key\n       ,t1.last_altered\n       ,t1.last_ddl\n   FROM snowflake.account_usage.table_storage_metrics t\n        JOIN snowflake.account_usage.tables t1\n          ON t.id=t1.table_id\n  WHERE 1=1\n        AND t1.table_catalog not in ('SNOWFLAKE')\n        AND \n            (\n             churn_pct>=40\n             OR\n             table_life_duration_hours<=24\n            )\n  ORDER BY total_size_gb desc ;",
      "id": "cell-65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§§ì€ ìƒëª… ì£¼ê¸°ì˜ í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸')\n\nHighChurn_ShortLived_df = dataframe_30\n\nprint(\"ë°ì´í„°ë² ì´ìŠ¤ë³„ í…Œì´ë¸” ìˆ˜\")\nfig_db = px.bar(HighChurn_ShortLived_df, x='DATABASE', color='DATABASE',\n             title='ë°ì´í„°ë² ì´ìŠ¤ë³„ í…Œì´ë¸” ìˆ˜',\n             labels={'DATABASE': 'ë°ì´í„°ë² ì´ìŠ¤', 'count': 'í…Œì´ë¸” ìˆ˜'})\nfig_db.update_layout(xaxis_tickangle=-45)\nfig_db.show()\n\nprint(\"ìŠ¤í‚¤ë§ˆë³„ í…Œì´ë¸” ìˆ˜\")\nfig_schema = px.bar(HighChurn_ShortLived_df, x='SCHEMA', color='SCHEMA',\n                 title='ìŠ¤í‚¤ë§ˆë³„ í…Œì´ë¸” ìˆ˜',\n                 labels={'SCHEMA': 'ìŠ¤í‚¤ë§ˆ', 'count': 'í…Œì´ë¸” ìˆ˜'})\nfig_schema.update_layout(xaxis_tickangle=-45)\nfig_schema.show()\n\nprint(\"í…Œì´ë¸”ë³„ ì´ í¬ê¸° (GB)\")\ndf_sorted = HighChurn_ShortLived_df.sort_values(by='TOTAL_SIZE_GB', ascending=False)\nfig_size = px.bar(df_sorted, y='DATABASE', x='TOTAL_SIZE_GB', color='TABLE_NAME',\n                 title='í…Œì´ë¸”ë³„ ì´ í¬ê¸° (GB)',\n                 labels={'DATABASE': 'ë°ì´í„°ë² ì´ìŠ¤', 'TOTAL_SIZE_GB': 'ì´ í¬ê¸° (GB)', 'TABLE_NAME': 'í…Œì´ë¸” ì´ë¦„'},\n                 orientation='h')\nfig_size.update_layout(yaxis={'categoryorder':'array', 'categoryarray': df_sorted['DATABASE'].unique()})\nfig_size.show()\n\nprint(\"í…Œì´ë¸” ì •ë³´\")\nfig_table = go.Figure(data=[go.Table(\n    header=dict(values=['DATABASE', 'SCHEMA', 'TABLE_NAME', 'DELETED','IS_TRANSIENT','LAST_ALTERED','LAST_DDL'],\n                align='left'),\n    cells=dict(values=[HighChurn_ShortLived_df['DATABASE'], HighChurn_ShortLived_df['SCHEMA'], HighChurn_ShortLived_df['TABLE_NAME'], HighChurn_ShortLived_df['DELETED'],HighChurn_ShortLived_df['IS_TRANSIENT'],HighChurn_ShortLived_df['LAST_ALTERED'], HighChurn_ShortLived_df['LAST_DDL']],\n               align='left'))\n])\nfig_table.update_layout(title='í…Œì´ë¸” ì´ë¦„ ë° ì´ í¬ê¸°')\nfig_table.show()",
      "id": "cell-66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_31"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_31\n-- Unused tables\nSELECT TABLE_CATALOG || '.' || TABLE_SCHEMA || '.' || TABLE_NAME AS TABLE_PATH\n      ,TABLE_CATALOG AS DATABASE\n      ,TABLE_SCHEMA AS SCHEMA\n      ,TABLE_NAME\n      ,BYTES\n      ,TO_NUMBER(BYTES / POWER(1024,3),10,2) AS GB\n      ,LAST_ALTERED AS LAST_USE\n      ,DATEDIFF('Day',LAST_USE,CURRENT_DATE) AS DAYS_SINCE_LAST_USE\n  FROM FROSTBYTE_TASTY_BYTES.INFORMATION_SCHEMA.TABLES\n WHERE DAYS_SINCE_LAST_USE > 7\n ORDER BY DATABASE, SCHEMA, TABLE_NAME, DAYS_SINCE_LAST_USE DESC;",
      "id": "cell-67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('7ì¼ ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•Šì€ í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸')\n\nUnusedTable_df = dataframe_31\n\ndf_sorted = UnusedTable_df.sort_values(by='GB', ascending=False)\nfig_size = px.bar(df_sorted, y='DATABASE', x='GB', color='TABLE_PATH',\n                 title='í…Œì´ë¸”ë³„ ì´ í¬ê¸° (GB)',\n                 labels={'DATABASE': 'ë°ì´í„°ë² ì´ìŠ¤', 'GB': 'ì´ í¬ê¸° (GB)', 'TABLE_PATH': 'í…Œì´ë¸” ì´ë¦„'},\n                 orientation='h')\nfig_size.update_layout(yaxis={'categoryorder':'array', 'categoryarray': df_sorted['DATABASE'].unique()})\nfig_size.show()\n\nfig_table = go.Figure(data=[go.Table(\n    header=dict(values=list(UnusedTable_df[['DATABASE', 'SCHEMA', 'TABLE_NAME','GB']].columns),\n                align='left'),\n    cells=dict(values=[UnusedTable_df['DATABASE'], UnusedTable_df['SCHEMA'], UnusedTable_df['TABLE_NAME'], UnusedTable_df['GB']],\n               align='left'))\n])\nfig_table.update_layout(title='í…Œì´ë¸” ì´ë¦„ ë° ì´ í¬ê¸°')\nfig_table.show()",
      "id": "cell-68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_32"
      },
      "outputs": [],
      "source": "%%sql -r dataframe_32\n-- Tables used in any query in the last 7 days\nWITH access_history as\n(   \nSELECT  \n       distinct split(base.value:objectName, '.')[0]::string as DATABASE\n       ,split(base.value:objectName, '.')[1]::string as SCHEMA\n       ,split(base.value:objectName, '.')[2]::string as TABLE_NAME\n  FROM snowflake.account_usage.access_history \n       ,lateral flatten (base_objects_accessed) base\n where query_start_time between current_date()-7 and current_date()\n)\nSELECT tbl.table_catalog||'.'||tbl.table_schema||'.'||tbl.table_name as FQ_table_name,\n       ah.database,\n       ah.schema,\n       ah.table_name,\n       TO_NUMBER(tbl.bytes / POWER(1024,3),10,2) AS GB\n  FROM snowflake.account_usage.tables tbl\n  LEFT JOIN access_history ah\n    ON tbl.table_name=ah.table_name\n   AND tbl.table_schema=ah.schema\n   AND tbl.table_catalog=ah.database\n WHERE ah.table_name is not NULL\n   AND tbl.deleted is null \n;",
      "id": "cell-69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "title('ì§€ë‚œ 7ì¼ê°„ ì¿¼ë¦¬ì—ì„œ ì‚¬ìš©ëœ í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸')\n\nTablesUsedInQuery_df = dataframe_32\n\ndf_sorted = TablesUsedInQuery_df.sort_values(by='GB', ascending=False)\nfig_size = px.bar(df_sorted, y='DATABASE', x='GB', color='TABLE_NAME',\n                 title='í…Œì´ë¸”ë³„ ì´ í¬ê¸° (GB)',\n                 labels={'DATABASE': 'ë°ì´í„°ë² ì´ìŠ¤', 'GB': 'ì´ í¬ê¸° (GB)', 'TABLE_NAME': 'í…Œì´ë¸” ì´ë¦„'},\n                 orientation='h')\nfig_size.update_layout(yaxis={'categoryorder':'array', 'categoryarray': df_sorted['DATABASE'].unique()})\nfig_size.show()\n\nfig_table = go.Figure(data=[go.Table(\n    header=dict(values=list(df_sorted[['DATABASE', 'SCHEMA', 'TABLE_NAME','GB']].columns),\n                align='left'),\n    cells=dict(values=[df_sorted['DATABASE'], df_sorted['SCHEMA'], df_sorted['TABLE_NAME'], df_sorted['GB']],\n               align='left'))\n])\nfig_table.update_layout(title='í…Œì´ë¸” ì´ë¦„ ë° ì´ í¬ê¸°')\nfig_table.show()",
      "id": "cell-70"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}